{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from konlpy.tag import Kkma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Analysis Variables:\n",
    "\n",
    "- Length of the string 텍스트 스트링의 길이 (for equal comparison)\n",
    "- Number of tokens 토큰 수\n",
    "- Number of sentences 문장 수\n",
    "- Tokens sorted by frequency 빈도 수로 정렬한 토큰 리스트\n",
    "- Sentence structure category sorted by frequency 빈도 수로 정렬한 문장 구조 카테고리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def get_string_length(text: str) -> int:\n",
    "    return(len(text))\n",
    "def get_num_tokens(tokens: list) -> int:\n",
    "    return(len(tokens))\n",
    "def find_sent_id(tokens: list) -> list:\n",
    "    ids = []\n",
    "    for i, tok in enumerate(tokens):\n",
    "        found = re.findall('[.?!]+', tok)\n",
    "        if(bool(found)):\n",
    "            ids.append(i)\n",
    "    return ids\n",
    "def split_sentence(sent_ids: list, tokens: list)-> list:  \n",
    "    sentences = []\n",
    "    if bool(sent_ids):\n",
    "        start = 0\n",
    "        for i in sent_ids:\n",
    "            sentences.append(tokens[start:i+1])\n",
    "            start = i+1\n",
    "    else:\n",
    "        sentences.append(tokens)\n",
    "    return sentences\n",
    "def split_pos_sentence(sent_ids: list, pos: list)-> list:  \n",
    "    sentences = []\n",
    "    if bool(sent_ids):\n",
    "        start = 0\n",
    "        for i in sent_ids:\n",
    "            sent_both = pos[start:i+1] #get the sentence\n",
    "            sent_tag = list(map(lambda tag: tag[1], sent_both))#get only the pos tags\n",
    "            sentences.append(sent_tag)#append\n",
    "            start = i+1\n",
    "    else:\n",
    "        sent_both = pos\n",
    "        sent_tag = list(map(lambda tag: tag[1], sent_both))#get only the pos tags\n",
    "        sentences.append(sent_tag)\n",
    "    return sentences\n",
    "def get_num_sentences(tokens:list, pos:list):\n",
    "    sent_ids = find_sent_id(tokens)\n",
    "    sentences = split_sentence(sent_ids, tokens)\n",
    "    pos_sentences = split_pos_sentence(sent_ids, pos)\n",
    "    return (len(sent_ids), sentences, pos_sentences)\n",
    "def get_num_token_sentences(sentences):\n",
    "    num_tokens = []\n",
    "    for sentence in sentences:\n",
    "        num_tokens.append(len(sentence))\n",
    "    return num_tokens\n",
    "def simple_analysis(data):\n",
    "    data['sentence_tokens'] = []\n",
    "    data['sentence_pos'] = []\n",
    "    all_analysis = {'string_length':[], 'num_tokens':[], 'num_sentences':[], 'num_token_sentences':[],}\n",
    "    for i in tqdm(range(len(data['text']))):\n",
    "        text = data['text'][i]\n",
    "        tokens = data['tokens'][i]\n",
    "        pos = data['pos'][i]\n",
    "        all_analysis['string_length'].append(get_string_length(text))\n",
    "        all_analysis['num_tokens'].append(get_num_tokens(tokens))\n",
    "        num_sentences, sentences, pos_sentences = get_num_sentences(tokens, pos)\n",
    "        num_token_sentences = get_num_token_sentences(sentences)\n",
    "        all_analysis['num_sentences'].append(num_sentences)\n",
    "        all_analysis['num_token_sentences'].append(num_token_sentences)\n",
    "        data['sentence_tokens'].append(sentences)\n",
    "        data['sentence_pos'].append(pos_sentences)\n",
    "    \n",
    "    return data, all_analysis\n",
    "def get_rel_freq(freq: dict):\n",
    "    freq = pd.Series(freq, dtype = \"float64\")\n",
    "    tot = freq.sum()\n",
    "    return (freq/tot).to_dict()\n",
    "def get_pos_freq(articles: list[list]) -> dict:\n",
    "    pos_freq = {}\n",
    "    pos_freq['s_freq'] = []\n",
    "    pos_freq['a_freq'] = []\n",
    "    pos_freq['s_rel_freq'] = []\n",
    "    pos_freq['a_rel_freq'] = []\n",
    "    for i in tqdm(range(len(articles))):\n",
    "        s_freq = []\n",
    "        s_rel_freq = []\n",
    "        article_freq = dict()\n",
    "        for sentence in articles[i]:\n",
    "            sentence_freq = dict(Counter(sentence))\n",
    "            s_freq.append(sentence_freq)\n",
    "            s_rel_freq.append(get_rel_freq(sentence_freq))\n",
    "            for (key, freq) in sentence_freq.items():\n",
    "                if key in article_freq.keys():\n",
    "                    article_freq[key] = article_freq[key] + freq\n",
    "                else:\n",
    "                    article_freq[key] = freq\n",
    "        pos_freq['s_freq'].append(s_freq)\n",
    "        pos_freq['s_rel_freq'].append(s_rel_freq)\n",
    "        pos_freq['a_freq'].append(article_freq)\n",
    "        pos_freq['a_rel_freq'].append(get_rel_freq(article_freq))\n",
    "        \n",
    "    return pos_freq\n",
    "\n",
    "def calRelPosition(sentence_pos, num_token_sentences):\n",
    "\n",
    "    for i, sentence in enumerate(sentence_pos):\n",
    "        pos = np.array(sentence)\n",
    "        result = {}\n",
    "        for tag in set(pos):\n",
    "            indices = np.where(pos == tag)\n",
    "            result[tag] = (indices/num_token_sentences[i]).tolist()\n",
    "           \n",
    "    return result\n",
    "\n",
    "def get_more_features(data, all_analysis):\n",
    "    all_analysis['token_variety'] = []\n",
    "    all_analysis['rel_position'] = []\n",
    "    for i in tqdm(range(len(data['pos']))):\n",
    "        num_token_sentences = np.array(all_analysis['num_token_sentences'][i])\n",
    "        num_token_type = np.array(list(map(len, all_analysis['pos_freq']['s_freq'][i])))\n",
    "        token_variety = (num_token_type / num_token_sentences).tolist()\n",
    "        all_analysis['token_variety'].append(token_variety)\n",
    "        sentence_pos = data['sentence_pos'][i]\n",
    "        rel_position = calRelPosition(sentence_pos, num_token_sentences)\n",
    "        all_analysis['rel_position'].append(rel_position)\n",
    "    return all_analysis\n",
    "\n",
    "\n",
    "def analysis(data):\n",
    "    data, all_analysis = simple_analysis(data)\n",
    "    all_analysis['pos_freq'] = get_pos_freq(data['sentence_pos'])\n",
    "    # all_analysis = get_more_features(data, all_analysis)\n",
    "    return data, all_analysis\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/lhi30/Haein/2023/YBIGTA/2023-2/DA/Writing_Advice/articles_ytn_analyzed.json'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"C:/Users/lhi30/Haein/2023/YBIGTA/2023-2/DA/Writing_Advice/articles_ytn_tokenized.json\"\n",
    "\"C:/Users/lhi30/Haein/2023/YBIGTA/2023-2/DA/Writing_Advice/articles_ytn_analyzed.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "a = data['sentence_pos'][1215]\n",
    "b = np.array(new_analysis['num_token_sentences'][1215])\n",
    "print(len(b))\n",
    "# calRelPosition(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = \"articles\"\n",
    "data_path = {\n",
    "    \"articles\":\"C:/Users/lhi30/Haein/2023/YBIGTA/2023-2/DA/Writing_Advice/tokenized_data/articles_ytn_tokenized.json\",\n",
    "    \"abstract\":\"C:/Users/lhi30/Haein/2023/YBIGTA/2023-2/DA/Writing_Advice/tokenized_data/abstract_tokenized.json\",\n",
    "    \"essay\":\"C:/Users/lhi30/Haein/2023/YBIGTA/2023-2/DA/Writing_Advice/tokenized_data/essay_tokenized.json\",\n",
    "    \"literature\":\"C:/Users/lhi30/Haein/2023/YBIGTA/2023-2/DA/Writing_Advice/tokenized_data/literature_tokenized.json\",\n",
    "    }\n",
    "\n",
    "with open(data_path[category]) as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# analysis = {\n",
    "#     \"articles\": analysis,\n",
    "#     \"abstract\": analysis,\n",
    "#     \"essay\": analysis,\n",
    "#     \"literature\":analysis,\n",
    "#     }\n",
    "\n",
    "# method = analysis[category]\n",
    "# data, all_analysis = method(data)\n",
    "\n",
    "# out_path = {\n",
    "#     \"articles\":\"C:/Users/lhi30/Haein/2023/YBIGTA/2023-2/DA/Writing_Advice/analyzed_data/article/articles_ytn_analysis.json\",\n",
    "#     \"abstract\":\"C:/Users/lhi30/Haein/2023/YBIGTA/2023-2/DA/Writing_Advice/analyzed_data/abstract/abstract_analysis.json\",\n",
    "#     \"essay\":\"C:/Users/lhi30/Haein/2023/YBIGTA/2023-2/DA/Writing_Advice/analyzed_data/essay/essay_analysis.json\",\n",
    "#     \"literature\":\"C:/Users/lhi30/Haein/2023/YBIGTA/2023-2/DA/Writing_Advice/analyzed_data/literature/literature_analysis.json\",\n",
    "# }\n",
    "# data_out_path = {\n",
    "#     \"articles\":\"C:/Users/lhi30/Haein/2023/YBIGTA/2023-2/DA/Writing_Advice/analyzed_data/article/articles_data.json\",\n",
    "#     \"abstract\":\"C:/Users/lhi30/Haein/2023/YBIGTA/2023-2/DA/Writing_Advice/analyzed_data/abstract/abstract_data.json\",\n",
    "#     \"essay\":\"C:/Users/lhi30/Haein/2023/YBIGTA/2023-2/DA/Writing_Advice/analyzed_data/essay/essay_data.json\",\n",
    "#     \"literature\":\"C:/Users/lhi30/Haein/2023/YBIGTA/2023-2/DA/Writing_Advice/analyzed_data/literature/literature_data.json\",\n",
    "# }\n",
    "# with open(out_path[category], \"w\") as outfile:\n",
    "#     json.dump(all_analysis, outfile)\n",
    "# with open(data_out_path[category], \"w\") as outfile:\n",
    "#     json.dump(data, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(data_path[category], \"w\") as outfile:\n",
    "#     json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_index(index_num, category):\n",
    "    data_path = {\n",
    "        \"articles\":\"C:/Users/lhi30/Haein/2023/YBIGTA/2023-2/DA/Writing_Advice/tokenized_data/articles_ytn_tokenized.json\",\n",
    "        \"abstract\":\"C:/Users/lhi30/Haein/2023/YBIGTA/2023-2/DA/Writing_Advice/tokenized_data/abstract_tokenized.json\",\n",
    "        \"essay\":\"C:/Users/lhi30/Haein/2023/YBIGTA/2023-2/DA/Writing_Advice/tokenized_data/essay_tokenized.json\",\n",
    "        \"literature\":\"C:/Users/lhi30/Haein/2023/YBIGTA/2023-2/DA/Writing_Advice/tokenized_data/literature_tokenized.json\",\n",
    "        }\n",
    "\n",
    "    with open(data_path[category]) as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    data['text'].pop(index_num)\n",
    "    data['tokens'].pop(index_num)\n",
    "    data['pos'].pop(index_num)\n",
    "    \n",
    "    with open(data_path[category], \"w\") as outfile:\n",
    "        json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_index(3000+1713, \"articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"온라인에서 화제가 된 소식을 전해드리는 '오늘 세 컷'튀르키예·시리아 강진 일주일째를 맞아 피해 현장에는 세계 각국에서 온 구조팀이 사투를 벌이고 있는데요.구조팀에 소속돼 임무를 수행하던 구조견들의 부상과 사망소식도 전해지고 있습니다.앞발에 붕대를 감고 잔해가 깔린 바닥에 앉아있는 구조견.우리 긴급구조대와 함께 튀르키예에 파견된 6살 토백이입니다.며칠 전 구조작업 도중 날카로운 물체에 찔려 다쳤지만, 응급처치를 받고 다시 현장에 투입됐습니다.짠하면서, 대견한 마음도 드는데요.우리 구조대는, 붕대 투혼 중인 토백이를 위해, 위험한 곳에선 직접 안아서 옮겨주며 구조작업을 진행하고 있다고 합니다.세상을 떠난 구조견도 있습니다.멕시코 구조견 프로테오는 현장에서 구조 활동 중 세상을 떠나 애도의 물결이 이어지기도 했는데요.누리꾼들은 이들의 활약에 고마움을 전하며 모두 건강히 돌아오길 기원했습니다.지난해 말 달로 떠난, 달 궤도선 다누리가 처음으로 달 가까이에서 촬영한 사진을 보내왔습니다.화면으로 함께 보시죠.달 표면에서, 웅덩이처럼 유독 크게 패인 이곳은 달에서 가장 큰 바다인, 폭풍의 바다인데요.면적이 한반도 18배 크기에 달합니다.지난 1966년 세계 첫 달 착륙선인 옛소련의 루나 9호가 착륙한 곳이기도 하죠.또, 여러 개의 크레이터가 모여 형성된 레이타 계곡과, 인류 최초로 달 표면 탐사가 이뤄진 '비의 바다'도 생생하게 담겼습니다.다누리는 이 밖에도, 달에서 본 지구의 신비롭고 다양한 모습도 함께 보내왔습니다.시운전을 마치고, 지난 4일부터 정상 운영에 착수한 다누리는, 올해 말까지 달 과학연구과 우주 인터넷 기술 검증 등 임무를 수행할 예정입니다.매년 아동학대 사건이 끊이지 않는데요.피해 아동 대부분은 원래 가정으로 다시 돌아가는 거로 나타났습니다.보건복지부의 '2021년 아동학대 주요통계'를 보면, 부모가 학대 가해자로 나타난 경우가 전체 아동학대 사건의 83.7%로 대부분이었고, 학대 장소도 대부분 가정이었는데요.정작 피해 아동 10명 중 8~9명은 당국의 신고 접수와 처리 뒤에도 분리 없이 다시 가정으로 돌아간 것으로 집계됐습니다.이 때문에 재학대 사례도 빈번하게 발생하고 있는데요.지난 2021년을 기준 아동학대 사례 중 또다시 신고가 접수된 재학대 사례는 전체 아동학대 건수 중 15%에 육박했습니다.더 큰 문제는 이 재학대 비중이 갈수록 늘고 있다는 겁니다.아동학대 가정 대한 상황 파악과 아동 안전에 대한 관리가 제대로 되지 않고 있는 것 아니냐는 지적이 나오고 있습니다.지금까지 '오늘 세 컷'이었습니다.\""
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category = \"articles\"\n",
    "data_path = {\n",
    "    \"articles\":\"C:/Users/lhi30/Haein/2023/YBIGTA/2023-2/DA/Writing_Advice/tokenized_data/articles_ytn_tokenized.json\",\n",
    "    \"abstract\":\"C:/Users/lhi30/Haein/2023/YBIGTA/2023-2/DA/Writing_Advice/tokenized_data/abstract_tokenized.json\",\n",
    "    \"essay\":\"C:/Users/lhi30/Haein/2023/YBIGTA/2023-2/DA/Writing_Advice/tokenized_data/essay_tokenized.json\",\n",
    "    \"literature\":\"C:/Users/lhi30/Haein/2023/YBIGTA/2023-2/DA/Writing_Advice/tokenized_data/literature_tokenized.json\",\n",
    "    }\n",
    "\n",
    "with open(data_path[category]) as json_file:\n",
    "    data = json.load(json_file)\n",
    "    \n",
    "data['text'][5477]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = {\n",
    "    'text': data['text'][3000:],\n",
    "    'tokens': data['tokens'][3000:],\n",
    "    'pos': data['pos'][3000:]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"'\", 'SS'],\n",
       " ['연기인', 'NNG'],\n",
       " ['생', 'XSN'],\n",
       " ['67', 'NR'],\n",
       " ['년', 'NNM'],\n",
       " ['차', 'NNG'],\n",
       " ['대배우', 'NNG'],\n",
       " [\"'\", 'SS'],\n",
       " ['이순', 'NNG'],\n",
       " ['재', 'NNG'],\n",
       " ['드라마', 'NNG'],\n",
       " ['부터', 'JX'],\n",
       " ['영화', 'NNG'],\n",
       " [',', 'SP'],\n",
       " ['광고', 'NNG'],\n",
       " ['까지', 'JX'],\n",
       " ['모두', 'MAG'],\n",
       " ['섭렵', 'NNG'],\n",
       " ['이순', 'NNG'],\n",
       " ['재', 'NNG'],\n",
       " ['\"', 'SS'],\n",
       " ['커피', 'NNG'],\n",
       " ['광고', 'NNG'],\n",
       " ['내가', 'NNG'],\n",
       " ['원조', 'NNG'],\n",
       " ['…', 'SE'],\n",
       " ['나', 'NP'],\n",
       " ['다음', 'NNG'],\n",
       " ['이', 'JKS'],\n",
       " ['안', 'MAG'],\n",
       " ['성기', 'NNG'],\n",
       " ['\"', 'SS'],\n",
       " [\"'\", 'SS'],\n",
       " ['거침없이', 'MAG'],\n",
       " ['하이', 'NNG'],\n",
       " ['킥', 'NNG'],\n",
       " [\"'\", 'SS'],\n",
       " ['에서', 'JKM'],\n",
       " ['파격적', 'NNG'],\n",
       " ['시트콤', 'NNG'],\n",
       " ['연기', 'NNG'],\n",
       " ['로', 'JKM'],\n",
       " ['대중적', 'NNG'],\n",
       " ['인기', 'NNG'],\n",
       " ['얻', 'VV'],\n",
       " ['어', 'ECD'],\n",
       " ['구', 'NNG'],\n",
       " ['순', 'NNG'],\n",
       " ['앞두', 'VV'],\n",
       " ['고', 'ECE'],\n",
       " ['연극', 'NNG'],\n",
       " [\"'\", 'SS'],\n",
       " ['갈매기', 'NNG'],\n",
       " [\"'\", 'SS'],\n",
       " ['연출', 'NNG'],\n",
       " ['에', 'JKM'],\n",
       " ['도전', 'NNG'],\n",
       " ['하', 'XSV'],\n",
       " ['ㄴ', 'ETD'],\n",
       " ['배우', 'NNG'],\n",
       " ['이순', 'NNG'],\n",
       " ['재', 'NNG'],\n",
       " ['\"', 'SS'],\n",
       " ['삶', 'NNG'],\n",
       " ['의', 'JKG'],\n",
       " ['원동력', 'NNG'],\n",
       " ['은', 'JX'],\n",
       " [\"'\", 'SS'],\n",
       " ['미완성', 'NNG'],\n",
       " [\"'\", 'SS'],\n",
       " ['\"', 'SS'],\n",
       " ['현', 'NNG'],\n",
       " ['역', 'NNG'],\n",
       " ['최', 'XPN'],\n",
       " ['고령', 'NNG'],\n",
       " ['배우', 'NNG'],\n",
       " ['의', 'JKG'],\n",
       " ['도전', 'NNG'],\n",
       " ['은', 'JX'],\n",
       " ['계속', 'NNG'],\n",
       " ['되', 'XSV'],\n",
       " ['ㄴ다', 'EFN']]"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 1713\n",
    "test_data['text'].pop(index)\n",
    "test_data['tokens'].pop(index)\n",
    "test_data['pos'].pop(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'아르헨티나에서 한인 남성이 동포 여성을 살해한 뒤 음독을 시도했다가 경찰에 붙잡혔습니다.현지시간 13일 중남미 매체인 엘누에보닷컴과 시티오안디노 등에 따르면 이날 오전 아르헨티나 멘도사주에서 긴급 전화에 \"독극물을 마셨다\"는 신고가 접수됐습니다.아르헨티나 경찰은 신고자를 페루파토 병원으로 옮겼는데, 한국 국적의 김 모 씨로 밝혀진 이 남성은 경찰에게 지난주에 같은 국적의 아내를 죽였다\"고 자백했습니다.경찰은 김 씨로부터 지난 9일쯤 함께 살던 동포 여성을 목 졸라 살해한 뒤 멘도사주 산마르틴 지역 돈페드로 농장 부근에 시신을 암매장했다는 추가 자백을 받아냈습니다.실제 김씨가 지목한 곳에서는 40대 여성으로 추정되는 시신이 발견된 것으로 파악됐습니다.김 씨는 현재 위중한 상태인 것으로 알려졌습니다.수사당국은 시신에 대해 부검을 하는 한편 법적 부부 여부 등 김 씨와 피해자 간 정확한 관계를 확인하고 있습니다.'"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['text'][2458]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3559/3559 [00:02<00:00, 1226.23it/s]\n",
      "100%|██████████| 3559/3559 [00:34<00:00, 104.56it/s]\n"
     ]
    }
   ],
   "source": [
    "new_data, new_analysis = analysis(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3559 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 2458/3559 [00:08<00:02, 465.16it/s]C:\\Users\\lhi30\\AppData\\Local\\Temp\\ipykernel_4584\\3951035193.py:119: RuntimeWarning: invalid value encountered in divide\n",
      "  token_variety = (num_token_type / num_token_sentences).tolist()\n",
      "100%|██████████| 3559/3559 [00:10<00:00, 323.72it/s]\n"
     ]
    }
   ],
   "source": [
    "all_analysis = get_more_features(new_data, new_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[47, 39, 17, 45, 99, 32, 41, 34, 12, 22, 28, 10, 28, 33]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = split_sentence(find_sent_id(tokens), tokens)\n",
    "get_num_token_sentences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'스토킹 살인' 전주환 징역 40년 선고\""
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = data['text'][1215]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str) -> dict:\n",
    "    kkma = Kkma()\n",
    "    tokens = kkma.morphs(text)\n",
    "    pos_tokens = kkma.pos(text)\n",
    "    return {'text': text, 'tokens': tokens, 'pos': pos_tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '본 연구는 소셜 빅데이터를 통해 국내에서 개봉된 일본 애니메이션 영화를 대상으로 흥행 요인을 분석하였다. 분석대상으로는 2017년 1월 국내에서 개봉한 너의 이름은.과 2021년 1월에 개봉한 귀멸의 칼날 무한열차편을 선정하였다. 분석 방법은 개봉 일자를 기준으로 유명 웹사이트에서 데이터를 수집하여, 추출된 키워드를 중심으로 텍스트 마이닝 기법인 의미연결망 분석과  분석, 감성 분석을 시행하였다. 분석 결과 두 작품은 소셜 미디어의 영향으로 온라인 구전 효과를 보았으며 관련 인물과 배급사, 제작 및 실사화, 이벤트, 상품성 등의 요인으로 화제가 되었다. 또한, 미디어믹스 성향이 강해 다양하게 소비할 수 있다는 강점도 드러났다. 이에 국내에서는 일본 애니메이션 영화를 소비하는 계층은 주로 블로그, 웹사이트, 를 통해 보는 경우가 많으며, 이 연구의 의의는 중심 키워드로부터 작품의 어떤 속성에 흥미가 있는지, 어떤 부분을 평가하고 있는지에 대해 분석했다는 점이다. ',\n",
       " 'tokens': ['보',\n",
       "  'ㄴ',\n",
       "  '연구',\n",
       "  '는',\n",
       "  '소',\n",
       "  '셜',\n",
       "  '빅',\n",
       "  '데이터',\n",
       "  '를',\n",
       "  '통하',\n",
       "  '어',\n",
       "  '국내',\n",
       "  '에서',\n",
       "  '개봉',\n",
       "  '되',\n",
       "  'ㄴ',\n",
       "  '일본',\n",
       "  '애니메이션',\n",
       "  '영화',\n",
       "  '를',\n",
       "  '대상',\n",
       "  '으로',\n",
       "  '흥행',\n",
       "  '요인',\n",
       "  '을',\n",
       "  '분석',\n",
       "  '하',\n",
       "  '였',\n",
       "  '다',\n",
       "  '.',\n",
       "  '분석대상',\n",
       "  '으로',\n",
       "  '는',\n",
       "  '2017',\n",
       "  '년',\n",
       "  '1',\n",
       "  '월',\n",
       "  '국내',\n",
       "  '에서',\n",
       "  '개봉',\n",
       "  '하',\n",
       "  'ㄴ',\n",
       "  '너',\n",
       "  '의',\n",
       "  '이름',\n",
       "  '은',\n",
       "  '.',\n",
       "  '과',\n",
       "  '2021',\n",
       "  '년',\n",
       "  '1',\n",
       "  '월',\n",
       "  '에',\n",
       "  '개봉',\n",
       "  '하',\n",
       "  'ㄴ',\n",
       "  '귀멸',\n",
       "  '의',\n",
       "  '칼날',\n",
       "  '무한',\n",
       "  '열차',\n",
       "  '편',\n",
       "  '을',\n",
       "  '선정',\n",
       "  '하',\n",
       "  '였',\n",
       "  '다',\n",
       "  '.',\n",
       "  '분',\n",
       "  '석',\n",
       "  '방법',\n",
       "  '은',\n",
       "  '개봉',\n",
       "  '일자',\n",
       "  '를',\n",
       "  '기준',\n",
       "  '으로',\n",
       "  '유명',\n",
       "  '웹',\n",
       "  '사이트',\n",
       "  '에서',\n",
       "  '데이터',\n",
       "  '를',\n",
       "  '수집',\n",
       "  '하',\n",
       "  '여',\n",
       "  ',',\n",
       "  '추출',\n",
       "  '되',\n",
       "  'ㄴ',\n",
       "  '키워드',\n",
       "  '를',\n",
       "  '중심',\n",
       "  '으로',\n",
       "  '텍스트',\n",
       "  '마이닝',\n",
       "  '기법',\n",
       "  '이',\n",
       "  'ㄴ',\n",
       "  '의미',\n",
       "  '연결망',\n",
       "  '분석',\n",
       "  '과',\n",
       "  '분석',\n",
       "  ',',\n",
       "  '감성',\n",
       "  '분석',\n",
       "  '을',\n",
       "  '시행',\n",
       "  '하',\n",
       "  '였',\n",
       "  '다',\n",
       "  '.',\n",
       "  '분석',\n",
       "  '결과',\n",
       "  '두',\n",
       "  '작품',\n",
       "  '은',\n",
       "  '소',\n",
       "  '셜',\n",
       "  '미디어',\n",
       "  '의',\n",
       "  '영향',\n",
       "  '으로',\n",
       "  '온라인',\n",
       "  '구전',\n",
       "  '효과',\n",
       "  '를',\n",
       "  '보',\n",
       "  '았',\n",
       "  '으며',\n",
       "  '관련',\n",
       "  '인물',\n",
       "  '과',\n",
       "  '배급사',\n",
       "  ',',\n",
       "  '제작',\n",
       "  '및',\n",
       "  '실',\n",
       "  '사화',\n",
       "  ',',\n",
       "  '이벤트',\n",
       "  ',',\n",
       "  '상품성',\n",
       "  '등',\n",
       "  '의',\n",
       "  '요인',\n",
       "  '으로',\n",
       "  '화제',\n",
       "  '가',\n",
       "  '되',\n",
       "  '었',\n",
       "  '다',\n",
       "  '.',\n",
       "  '또',\n",
       "  '한',\n",
       "  ',',\n",
       "  '미디어',\n",
       "  '믹스',\n",
       "  '성향',\n",
       "  '이',\n",
       "  '강하',\n",
       "  '어',\n",
       "  '다양',\n",
       "  '하',\n",
       "  '게',\n",
       "  '소비',\n",
       "  '하',\n",
       "  'ㄹ',\n",
       "  '수',\n",
       "  '있',\n",
       "  '다는',\n",
       "  '강점',\n",
       "  '도',\n",
       "  '드러나',\n",
       "  '었',\n",
       "  '다',\n",
       "  '.',\n",
       "  '이에',\n",
       "  '국내',\n",
       "  '에서',\n",
       "  '는',\n",
       "  '일본',\n",
       "  '애니메이션',\n",
       "  '영화',\n",
       "  '를',\n",
       "  '소비',\n",
       "  '하',\n",
       "  '는',\n",
       "  '계층',\n",
       "  '은',\n",
       "  '주로',\n",
       "  '블',\n",
       "  'ㄹ',\n",
       "  '로그',\n",
       "  ',',\n",
       "  '웹사이트',\n",
       "  ',',\n",
       "  '를',\n",
       "  '통하',\n",
       "  '어',\n",
       "  '보',\n",
       "  '는',\n",
       "  '경우',\n",
       "  '가',\n",
       "  '많',\n",
       "  '으며',\n",
       "  ',',\n",
       "  '이',\n",
       "  '연구',\n",
       "  '의',\n",
       "  '의의',\n",
       "  '는',\n",
       "  '중심',\n",
       "  '키워드',\n",
       "  '로',\n",
       "  '부터',\n",
       "  '작품',\n",
       "  '의',\n",
       "  '어떤',\n",
       "  '속성',\n",
       "  '에',\n",
       "  '흥미',\n",
       "  '가',\n",
       "  '있',\n",
       "  '는지',\n",
       "  ',',\n",
       "  '어떤',\n",
       "  '부분',\n",
       "  '을',\n",
       "  '평가',\n",
       "  '하',\n",
       "  '고',\n",
       "  '있',\n",
       "  '는',\n",
       "  '지에',\n",
       "  '대하',\n",
       "  '어',\n",
       "  '분석',\n",
       "  '하',\n",
       "  '었',\n",
       "  '다는',\n",
       "  '점',\n",
       "  '이',\n",
       "  '다',\n",
       "  '.'],\n",
       " 'pos': [('보', 'VV'),\n",
       "  ('ㄴ', 'ETD'),\n",
       "  ('연구', 'NNG'),\n",
       "  ('는', 'JX'),\n",
       "  ('소', 'NNG'),\n",
       "  ('셜', 'UN'),\n",
       "  ('빅', 'NNG'),\n",
       "  ('데이터', 'NNG'),\n",
       "  ('를', 'JKO'),\n",
       "  ('통하', 'VV'),\n",
       "  ('어', 'ECS'),\n",
       "  ('국내', 'NNG'),\n",
       "  ('에서', 'JKM'),\n",
       "  ('개봉', 'NNG'),\n",
       "  ('되', 'XSV'),\n",
       "  ('ㄴ', 'ETD'),\n",
       "  ('일본', 'NNG'),\n",
       "  ('애니메이션', 'NNG'),\n",
       "  ('영화', 'NNG'),\n",
       "  ('를', 'JKO'),\n",
       "  ('대상', 'NNG'),\n",
       "  ('으로', 'JKM'),\n",
       "  ('흥행', 'NNG'),\n",
       "  ('요인', 'NNG'),\n",
       "  ('을', 'JKO'),\n",
       "  ('분석', 'NNG'),\n",
       "  ('하', 'XSV'),\n",
       "  ('였', 'EPT'),\n",
       "  ('다', 'EFN'),\n",
       "  ('.', 'SF'),\n",
       "  ('분석대상', 'NNG'),\n",
       "  ('으로', 'JKM'),\n",
       "  ('는', 'JX'),\n",
       "  ('2017', 'NR'),\n",
       "  ('년', 'NNM'),\n",
       "  ('1', 'NR'),\n",
       "  ('월', 'NNM'),\n",
       "  ('국내', 'NNG'),\n",
       "  ('에서', 'JKM'),\n",
       "  ('개봉', 'NNG'),\n",
       "  ('하', 'XSV'),\n",
       "  ('ㄴ', 'ETD'),\n",
       "  ('너', 'NP'),\n",
       "  ('의', 'JKG'),\n",
       "  ('이름', 'NNG'),\n",
       "  ('은', 'JX'),\n",
       "  ('.', 'SF'),\n",
       "  ('과', 'JKM'),\n",
       "  ('2021', 'NR'),\n",
       "  ('년', 'NNM'),\n",
       "  ('1', 'NR'),\n",
       "  ('월', 'NNM'),\n",
       "  ('에', 'JKM'),\n",
       "  ('개봉', 'NNG'),\n",
       "  ('하', 'XSV'),\n",
       "  ('ㄴ', 'ETD'),\n",
       "  ('귀멸', 'UN'),\n",
       "  ('의', 'JKG'),\n",
       "  ('칼날', 'NNG'),\n",
       "  ('무한', 'NNG'),\n",
       "  ('열차', 'NNG'),\n",
       "  ('편', 'NNG'),\n",
       "  ('을', 'JKO'),\n",
       "  ('선정', 'NNG'),\n",
       "  ('하', 'XSV'),\n",
       "  ('였', 'EPT'),\n",
       "  ('다', 'EFN'),\n",
       "  ('.', 'SF'),\n",
       "  ('분', 'NR'),\n",
       "  ('석', 'NNM'),\n",
       "  ('방법', 'NNG'),\n",
       "  ('은', 'JX'),\n",
       "  ('개봉', 'NNG'),\n",
       "  ('일자', 'NNG'),\n",
       "  ('를', 'JKO'),\n",
       "  ('기준', 'NNG'),\n",
       "  ('으로', 'JKM'),\n",
       "  ('유명', 'NNG'),\n",
       "  ('웹', 'NNG'),\n",
       "  ('사이트', 'NNG'),\n",
       "  ('에서', 'JKM'),\n",
       "  ('데이터', 'NNG'),\n",
       "  ('를', 'JKO'),\n",
       "  ('수집', 'NNG'),\n",
       "  ('하', 'XSV'),\n",
       "  ('여', 'ECS'),\n",
       "  (',', 'SP'),\n",
       "  ('추출', 'NNG'),\n",
       "  ('되', 'XSV'),\n",
       "  ('ㄴ', 'ETD'),\n",
       "  ('키워드', 'NNG'),\n",
       "  ('를', 'JKO'),\n",
       "  ('중심', 'NNG'),\n",
       "  ('으로', 'JKM'),\n",
       "  ('텍스트', 'NNG'),\n",
       "  ('마이닝', 'NNG'),\n",
       "  ('기법', 'NNG'),\n",
       "  ('이', 'VCP'),\n",
       "  ('ㄴ', 'ETD'),\n",
       "  ('의미', 'NNG'),\n",
       "  ('연결망', 'NNG'),\n",
       "  ('분석', 'NNG'),\n",
       "  ('과', 'JKM'),\n",
       "  ('분석', 'NNG'),\n",
       "  (',', 'SP'),\n",
       "  ('감성', 'NNG'),\n",
       "  ('분석', 'NNG'),\n",
       "  ('을', 'JKO'),\n",
       "  ('시행', 'NNG'),\n",
       "  ('하', 'XSV'),\n",
       "  ('였', 'EPT'),\n",
       "  ('다', 'EFN'),\n",
       "  ('.', 'SF'),\n",
       "  ('분석', 'NNG'),\n",
       "  ('결과', 'NNG'),\n",
       "  ('두', 'MDN'),\n",
       "  ('작품', 'NNG'),\n",
       "  ('은', 'JX'),\n",
       "  ('소', 'NNG'),\n",
       "  ('셜', 'UN'),\n",
       "  ('미디어', 'NNG'),\n",
       "  ('의', 'JKG'),\n",
       "  ('영향', 'NNG'),\n",
       "  ('으로', 'JKM'),\n",
       "  ('온라인', 'NNG'),\n",
       "  ('구전', 'NNG'),\n",
       "  ('효과', 'NNG'),\n",
       "  ('를', 'JKO'),\n",
       "  ('보', 'VV'),\n",
       "  ('았', 'EPT'),\n",
       "  ('으며', 'ECE'),\n",
       "  ('관련', 'NNG'),\n",
       "  ('인물', 'NNG'),\n",
       "  ('과', 'JC'),\n",
       "  ('배급사', 'NNG'),\n",
       "  (',', 'SP'),\n",
       "  ('제작', 'NNG'),\n",
       "  ('및', 'MAG'),\n",
       "  ('실', 'NNG'),\n",
       "  ('사화', 'NNG'),\n",
       "  (',', 'SP'),\n",
       "  ('이벤트', 'NNG'),\n",
       "  (',', 'SP'),\n",
       "  ('상품성', 'NNG'),\n",
       "  ('등', 'NNB'),\n",
       "  ('의', 'JKG'),\n",
       "  ('요인', 'NNG'),\n",
       "  ('으로', 'JKM'),\n",
       "  ('화제', 'NNG'),\n",
       "  ('가', 'JKC'),\n",
       "  ('되', 'VV'),\n",
       "  ('었', 'EPT'),\n",
       "  ('다', 'EFN'),\n",
       "  ('.', 'SF'),\n",
       "  ('또', 'MAG'),\n",
       "  ('한', 'NNG'),\n",
       "  (',', 'SP'),\n",
       "  ('미디어', 'NNG'),\n",
       "  ('믹스', 'NNG'),\n",
       "  ('성향', 'NNG'),\n",
       "  ('이', 'JKS'),\n",
       "  ('강하', 'VV'),\n",
       "  ('어', 'ECS'),\n",
       "  ('다양', 'NNG'),\n",
       "  ('하', 'XSV'),\n",
       "  ('게', 'ECD'),\n",
       "  ('소비', 'NNG'),\n",
       "  ('하', 'XSV'),\n",
       "  ('ㄹ', 'ETD'),\n",
       "  ('수', 'NNB'),\n",
       "  ('있', 'VV'),\n",
       "  ('다는', 'ETD'),\n",
       "  ('강점', 'NNG'),\n",
       "  ('도', 'JX'),\n",
       "  ('드러나', 'VV'),\n",
       "  ('었', 'EPT'),\n",
       "  ('다', 'EFN'),\n",
       "  ('.', 'SF'),\n",
       "  ('이에', 'MAG'),\n",
       "  ('국내', 'NNG'),\n",
       "  ('에서', 'JKM'),\n",
       "  ('는', 'JX'),\n",
       "  ('일본', 'NNG'),\n",
       "  ('애니메이션', 'NNG'),\n",
       "  ('영화', 'NNG'),\n",
       "  ('를', 'JKO'),\n",
       "  ('소비', 'NNG'),\n",
       "  ('하', 'XSV'),\n",
       "  ('는', 'ETD'),\n",
       "  ('계층', 'NNG'),\n",
       "  ('은', 'JX'),\n",
       "  ('주로', 'MAG'),\n",
       "  ('블', 'VV'),\n",
       "  ('ㄹ', 'ETD'),\n",
       "  ('로그', 'NNG'),\n",
       "  (',', 'SP'),\n",
       "  ('웹사이트', 'NNG'),\n",
       "  (',', 'SP'),\n",
       "  ('를', 'JKO'),\n",
       "  ('통하', 'VV'),\n",
       "  ('어', 'ECS'),\n",
       "  ('보', 'VXV'),\n",
       "  ('는', 'ETD'),\n",
       "  ('경우', 'NNG'),\n",
       "  ('가', 'JKS'),\n",
       "  ('많', 'VA'),\n",
       "  ('으며', 'ECE'),\n",
       "  (',', 'SP'),\n",
       "  ('이', 'MDT'),\n",
       "  ('연구', 'NNG'),\n",
       "  ('의', 'JKG'),\n",
       "  ('의의', 'NNG'),\n",
       "  ('는', 'JX'),\n",
       "  ('중심', 'NNG'),\n",
       "  ('키워드', 'NNG'),\n",
       "  ('로', 'JKM'),\n",
       "  ('부터', 'JX'),\n",
       "  ('작품', 'NNG'),\n",
       "  ('의', 'JKG'),\n",
       "  ('어떤', 'MDT'),\n",
       "  ('속성', 'NNG'),\n",
       "  ('에', 'JKM'),\n",
       "  ('흥미', 'NNG'),\n",
       "  ('가', 'JKS'),\n",
       "  ('있', 'VV'),\n",
       "  ('는지', 'ECS'),\n",
       "  (',', 'SP'),\n",
       "  ('어떤', 'MDT'),\n",
       "  ('부분', 'NNG'),\n",
       "  ('을', 'JKO'),\n",
       "  ('평가', 'NNG'),\n",
       "  ('하', 'XSV'),\n",
       "  ('고', 'ECE'),\n",
       "  ('있', 'VXV'),\n",
       "  ('는', 'ETD'),\n",
       "  ('지에', 'NNG'),\n",
       "  ('대하', 'VV'),\n",
       "  ('어', 'ECS'),\n",
       "  ('분석', 'NNG'),\n",
       "  ('하', 'XSV'),\n",
       "  ('었', 'EPT'),\n",
       "  ('다는', 'ETD'),\n",
       "  ('점', 'NNG'),\n",
       "  ('이', 'VCP'),\n",
       "  ('다', 'EFN'),\n",
       "  ('.', 'SF')]}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = text_analysis(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_string_length(text: str) -> int:\n",
    "    return(len(text))\n",
    "def get_num_tokens(tokens: list) -> int:\n",
    "    return(len(tokens))\n",
    "def find_sent_id(tokens: list) -> list:\n",
    "    ids = []\n",
    "    for i, tok in enumerate(tokens):\n",
    "        found = re.findall('[.?!]+', tok)\n",
    "        if(bool(found)):\n",
    "            ids.append(i)\n",
    "    return ids\n",
    "def split_sentence(sent_ids: list, tokens: list)-> list:  \n",
    "    sentences = []\n",
    "    start = 0\n",
    "    for i in sent_ids:\n",
    "        sentences.append(tokens[start:i+1])\n",
    "        start = i+1\n",
    "    return sentences\n",
    "def split_pos_sentence(sent_ids: list, pos: list)-> list:  \n",
    "    sentences = []\n",
    "    start = 0\n",
    "    for i in sent_ids:\n",
    "        sent_both = pos[start:i+1] #get the sentence\n",
    "        sent_tag = list(map(lambda tag: tag[1], sent_both))#get only the pos tags\n",
    "        sentences.append(sent_tag)#append\n",
    "        start = i+1\n",
    "    return sentences\n",
    "def get_num_sentences(tokens:list, pos:list):\n",
    "    sent_ids = find_sent_id(tokens)\n",
    "    sentences = split_sentence(sent_ids, tokens)\n",
    "    pos_sentences = split_pos_sentence(sent_ids, pos)\n",
    "    return len(sent_ids), sentences, pos_sentences\n",
    "def get_num_token_sentences(sentences: list) -> list:\n",
    "    num_tokens = []\n",
    "    for sentence in sentences:\n",
    "        num_tokens.append(len(sentence))\n",
    "    return num_tokens\n",
    "\n",
    "def simple_analysis(data):\n",
    "    \n",
    "    text = data['text']\n",
    "    tokens = data['tokens']\n",
    "    pos = data['pos']\n",
    "\n",
    "    all_analysis = {}\n",
    "\n",
    "    all_analysis['string_length'] = get_string_length(text)\n",
    "    all_analysis['num_tokens'] = get_num_tokens(tokens)\n",
    "    num_sentences, sentences, pos_sentences = get_num_sentences(tokens, pos)\n",
    "    num_token_sentences = get_num_token_sentences(sentences)\n",
    "    all_analysis['num_sentences'] = num_sentences\n",
    "    all_analysis['num_token_sentences'] = num_token_sentences\n",
    "    data['sentence_tokens'] = sentences\n",
    "    data['sentence_pos'] = pos_sentences\n",
    "    \n",
    "    return data, all_analysis\n",
    "def get_rel_freq(freq: dict):\n",
    "    freq = pd.Series(freq, dtype = \"float64\")\n",
    "    tot = freq.sum()\n",
    "    return (freq/tot).to_dict()\n",
    "def get_pos_freq(article: list[list]) -> dict:\n",
    "    pos_freq = {}\n",
    "    s_freq = []\n",
    "    s_rel_freq = []\n",
    "    article_freq = dict()\n",
    "\n",
    "    for sentence in article:\n",
    "        sentence_freq = dict(Counter(sentence))\n",
    "        s_freq.append(sentence_freq)\n",
    "        s_rel_freq.append(get_rel_freq(sentence_freq))\n",
    "        for (key, freq) in sentence_freq.items():\n",
    "            if key in article_freq.keys():\n",
    "                article_freq[key] = article_freq[key] + freq\n",
    "            else:\n",
    "                article_freq[key] = freq\n",
    "\n",
    "    pos_freq['s_freq'] = s_freq\n",
    "    pos_freq['s_rel_freq'] = s_rel_freq\n",
    "    pos_freq['a_freq'] = article_freq\n",
    "    pos_freq['a_rel_freq'] = get_rel_freq(article_freq)\n",
    "        \n",
    "    return pos_freq \n",
    "\n",
    "def analysis(data):\n",
    "    data, all_analysis = simple_analysis(data)\n",
    "    all_analysis['pos_freq'] = get_pos_freq(data['sentence_pos'])\n",
    "    return data, all_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['text', 'tokens', 'pos', 'sentence_tokens', 'sentence_pos'])\n",
      "dict_keys(['string_length', 'num_tokens', 'num_sentences', 'num_token_sentences', 'pos_freq'])\n"
     ]
    }
   ],
   "source": [
    "data, all_analysis = analysis(new_text)\n",
    "print(data.keys())\n",
    "print(all_analysis.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = {\n",
    "    'text': text,\n",
    "    'category': 'abstract'\n",
    "    }\n",
    "with open(\"C:/Users/lhi30/Haein/2023/YBIGTA/2023-2/DA/Writing_Advice/new_input/new_input.json\", \"w\") as outfile:\n",
    "    json.dump(example, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '본 연구는 소셜 빅데이터를 통해 국내에서 개봉된 일본 애니메이션 영화를 대상으로 흥행 요인을 분석하였다. 분석대상으로는 2017년 1월 국내에서 개봉한 너의 이름은.과 2021년 1월에 개봉한 귀멸의 칼날 무한열차편을 선정하였다. 분석 방법은 개봉 일자를 기준으로 유명 웹사이트에서 데이터를 수집하여, 추출된 키워드를 중심으로 텍스트 마이닝 기법인 의미연결망 분석과  분석, 감성 분석을 시행하였다. 분석 결과 두 작품은 소셜 미디어의 영향으로 온라인 구전 효과를 보았으며 관련 인물과 배급사, 제작 및 실사화, 이벤트, 상품성 등의 요인으로 화제가 되었다. 또한, 미디어믹스 성향이 강해 다양하게 소비할 수 있다는 강점도 드러났다. 이에 국내에서는 일본 애니메이션 영화를 소비하는 계층은 주로 블로그, 웹사이트, 를 통해 보는 경우가 많으며, 이 연구의 의의는 중심 키워드로부터 작품의 어떤 속성에 흥미가 있는지, 어떤 부분을 평가하고 있는지에 대해 분석했다는 점이다. ',\n",
       " 'category': 'abstract'}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"C:/Users/lhi30/Haein/2023/YBIGTA/2023-2/DA/Writing_Advice/new_input/new_input.json\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = data['text']\n",
    "tokenized = tokenize(text)\n",
    "data, new_analysis = analysis(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['text', 'tokens', 'pos', 'sentence_tokens', 'sentence_pos'])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VV</th>\n",
       "      <th>ETD</th>\n",
       "      <th>NNG</th>\n",
       "      <th>JX</th>\n",
       "      <th>UN</th>\n",
       "      <th>JKO</th>\n",
       "      <th>ECS</th>\n",
       "      <th>JKM</th>\n",
       "      <th>XSV</th>\n",
       "      <th>EPT</th>\n",
       "      <th>...</th>\n",
       "      <th>ECE</th>\n",
       "      <th>JC</th>\n",
       "      <th>MAG</th>\n",
       "      <th>NNB</th>\n",
       "      <th>JKC</th>\n",
       "      <th>JKS</th>\n",
       "      <th>ECD</th>\n",
       "      <th>VXV</th>\n",
       "      <th>VA</th>\n",
       "      <th>MDT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.044715</td>\n",
       "      <td>0.052846</td>\n",
       "      <td>0.373984</td>\n",
       "      <td>0.04065</td>\n",
       "      <td>0.012195</td>\n",
       "      <td>0.04878</td>\n",
       "      <td>0.02439</td>\n",
       "      <td>0.060976</td>\n",
       "      <td>0.052846</td>\n",
       "      <td>0.028455</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012195</td>\n",
       "      <td>0.004065</td>\n",
       "      <td>0.01626</td>\n",
       "      <td>0.00813</td>\n",
       "      <td>0.004065</td>\n",
       "      <td>0.012195</td>\n",
       "      <td>0.004065</td>\n",
       "      <td>0.00813</td>\n",
       "      <td>0.004065</td>\n",
       "      <td>0.012195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         VV       ETD       NNG       JX        UN      JKO      ECS  \\\n",
       "0  0.044715  0.052846  0.373984  0.04065  0.012195  0.04878  0.02439   \n",
       "\n",
       "        JKM       XSV       EPT  ...       ECE        JC      MAG      NNB  \\\n",
       "0  0.060976  0.052846  0.028455  ...  0.012195  0.004065  0.01626  0.00813   \n",
       "\n",
       "        JKC       JKS       ECD      VXV        VA       MDT  \n",
       "0  0.004065  0.012195  0.004065  0.00813  0.004065  0.012195  \n",
       "\n",
       "[1 rows x 29 columns]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(pd.Series(new_analysis['pos_freq']['a_rel_freq'])).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# calculate number of tokens used / number of tokens in sentence\n",
    "def calTokenVariety(new_analysis):\n",
    "    num_token_type = np.array(list(map(len, new_analysis['pos_freq']['s_freq'])))\n",
    "    num_token_sentences = np.array(new_analysis['num_token_sentences'])\n",
    "    return num_token_type / num_token_sentences\n",
    "# def calRelPosition(new_analysis, data):\n",
    "#     sentence_pos = data['sentence_pos']\n",
    "#     num_token_sentences = np.array(new_analysis['num_token_sentences'])\n",
    "#     for i, sentence in enumerate(sentence_pos):\n",
    "#         pos = np.array(sentence)\n",
    "#         result = {}\n",
    "#         for tag in set(pos):\n",
    "#             indices = np.where(pos == tag)\n",
    "#             result[tag] = indices/num_token_sentences[i]\n",
    "\n",
    "#     return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calRelPosition(sentence_pos, num_token_sentences):\n",
    "    for i, sentence in enumerate(sentence_pos):\n",
    "        pos = np.array(sentence)\n",
    "        result = {}\n",
    "        for tag in set(pos):\n",
    "            indices = np.where(pos == tag)\n",
    "            result[tag] = (indices/num_token_sentences[i]).tolist()\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 2,  4,  6,  7, 11, 13, 16, 17, 18, 20, 22, 23, 25], dtype=int64),)\n",
      "['VV', 'ETD', 'NNG', 'JX', 'NNG', 'UN', 'NNG', 'NNG', 'JKO', 'VV', 'ECS', 'NNG', 'JKM', 'NNG', 'XSV', 'ETD', 'NNG', 'NNG', 'NNG', 'JKO', 'NNG', 'JKM', 'NNG', 'NNG', 'JKO', 'NNG', 'XSV', 'EPT', 'EFN', 'SF']\n"
     ]
    }
   ],
   "source": [
    "a = np.array(data['sentence_pos'][0])\n",
    "print(np.where(a == a[2]))\n",
    "print(data['sentence_pos'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['YTN',\n",
       " '라디오',\n",
       " '(',\n",
       " 'FM',\n",
       " '94.5',\n",
       " ')',\n",
       " 'YTN',\n",
       " '\\xa0',\n",
       " '뉴스',\n",
       " 'FM',\n",
       " '\\xa0',\n",
       " '슬기',\n",
       " '롭',\n",
       " 'ㄴ',\n",
       " '라디오',\n",
       " '생활',\n",
       " '□',\n",
       " '\\xa0',\n",
       " '방송',\n",
       " '일시',\n",
       " '\\xa0',\n",
       " ':',\n",
       " '2023',\n",
       " '년',\n",
       " '2',\n",
       " '월',\n",
       " '20',\n",
       " '일',\n",
       " '(',\n",
       " '월요',\n",
       " '일',\n",
       " ')',\n",
       " '□',\n",
       " '\\xa0',\n",
       " '진',\n",
       " '행',\n",
       " '\\xa0',\n",
       " ':',\n",
       " '\\xa0',\n",
       " '이',\n",
       " '현웅',\n",
       " '아나운서',\n",
       " '□',\n",
       " '\\xa0',\n",
       " '출연',\n",
       " ':',\n",
       " '최',\n",
       " '영',\n",
       " '묵',\n",
       " '경기도',\n",
       " '가족',\n",
       " '다문화',\n",
       " '과',\n",
       " '과장',\n",
       " '*',\n",
       " '\\xa0',\n",
       " '아래',\n",
       " '텍스트',\n",
       " '는',\n",
       " '실제',\n",
       " '방송',\n",
       " '내용',\n",
       " '과',\n",
       " '차이',\n",
       " '가',\n",
       " '있',\n",
       " '을',\n",
       " '수',\n",
       " '있',\n",
       " '으니',\n",
       " '보다',\n",
       " '정확',\n",
       " '하',\n",
       " 'ㄴ',\n",
       " '내용',\n",
       " '은',\n",
       " '방송',\n",
       " '으로',\n",
       " '확인',\n",
       " '하',\n",
       " '시',\n",
       " '기',\n",
       " '바라',\n",
       " 'ㅂ니다',\n",
       " '.',\n",
       " '◇',\n",
       " '\\xa0',\n",
       " '이',\n",
       " '현웅',\n",
       " '아나운서',\n",
       " '(',\n",
       " '이하',\n",
       " '이',\n",
       " '현웅',\n",
       " ')',\n",
       " ':',\n",
       " '생활',\n",
       " '백서',\n",
       " ',',\n",
       " '월요일',\n",
       " '은',\n",
       " '경기도',\n",
       " '와',\n",
       " '함께',\n",
       " '하',\n",
       " 'ㅂ니다',\n",
       " '.',\n",
       " '혼',\n",
       " '자',\n",
       " '있',\n",
       " '을',\n",
       " '때',\n",
       " '갑자기',\n",
       " '아프',\n",
       " '게',\n",
       " '되',\n",
       " '면',\n",
       " '병원',\n",
       " '갈',\n",
       " '는',\n",
       " '길',\n",
       " '도',\n",
       " '멀',\n",
       " '게',\n",
       " '느껴지',\n",
       " '고',\n",
       " '한편',\n",
       " '으로',\n",
       " '는',\n",
       " '서럽',\n",
       " 'ㄴ',\n",
       " '마음',\n",
       " '까',\n",
       " '지',\n",
       " '도',\n",
       " '들',\n",
       " '곤',\n",
       " '하',\n",
       " '는데요',\n",
       " '.',\n",
       " '이렇게',\n",
       " '아프',\n",
       " 'ㄹ',\n",
       " '때',\n",
       " '더',\n",
       " '힘들',\n",
       " 'ㄴ',\n",
       " '1',\n",
       " '인',\n",
       " '가구',\n",
       " '를',\n",
       " '위하',\n",
       " '어',\n",
       " '경기도',\n",
       " '가',\n",
       " '1',\n",
       " '인',\n",
       " '가구',\n",
       " '병원',\n",
       " '안심',\n",
       " '동행',\n",
       " '사업',\n",
       " '을',\n",
       " '추진',\n",
       " '하',\n",
       " 'ㄴ다고',\n",
       " '하',\n",
       " 'ㅂ니다',\n",
       " '.',\n",
       " '자세',\n",
       " '하',\n",
       " 'ㄴ',\n",
       " '내용',\n",
       " ',',\n",
       " '최',\n",
       " '영',\n",
       " '묵',\n",
       " '경기도',\n",
       " '가족',\n",
       " '다문화',\n",
       " '과장',\n",
       " '연결',\n",
       " '하',\n",
       " '어',\n",
       " '알아보',\n",
       " '겠',\n",
       " '습니다',\n",
       " '.',\n",
       " '안녕하',\n",
       " '세요',\n",
       " '?',\n",
       " '◆',\n",
       " '최',\n",
       " '영',\n",
       " '묵',\n",
       " '경기도',\n",
       " '가족',\n",
       " '다문화',\n",
       " '과',\n",
       " '과장',\n",
       " '(',\n",
       " '이하',\n",
       " '최',\n",
       " '영',\n",
       " '묵',\n",
       " ')',\n",
       " ':',\n",
       " '안녕',\n",
       " '하',\n",
       " '세요',\n",
       " '.',\n",
       " '경기도',\n",
       " '가족',\n",
       " '다문화',\n",
       " '과장',\n",
       " '최',\n",
       " '영',\n",
       " '묵이',\n",
       " 'ㅂ니다',\n",
       " '.',\n",
       " '◇',\n",
       " '\\xa0',\n",
       " '이',\n",
       " '현웅',\n",
       " ':',\n",
       " '경기도',\n",
       " '에서',\n",
       " '올해',\n",
       " '처음',\n",
       " '으로',\n",
       " '‘',\n",
       " '1',\n",
       " '인',\n",
       " '가구',\n",
       " '병원',\n",
       " '안심',\n",
       " '동행',\n",
       " '사업',\n",
       " '’',\n",
       " '을',\n",
       " '추진',\n",
       " '하',\n",
       " 'ㄴ다고',\n",
       " '하',\n",
       " '는데',\n",
       " ',',\n",
       " '어떤',\n",
       " '사업',\n",
       " '이',\n",
       " 'ㄴ지',\n",
       " '간략',\n",
       " '하',\n",
       " 'ㄴ',\n",
       " '소개',\n",
       " '부탁',\n",
       " '드리',\n",
       " 'ㅂ니다',\n",
       " '.',\n",
       " '◆',\n",
       " '최',\n",
       " '영',\n",
       " '묵',\n",
       " ':',\n",
       " '‘',\n",
       " '1',\n",
       " '인',\n",
       " '가구',\n",
       " '병원',\n",
       " '안심',\n",
       " '동행',\n",
       " '사업',\n",
       " '’',\n",
       " '은',\n",
       " '올해',\n",
       " '경기도',\n",
       " '의',\n",
       " '대표적',\n",
       " '이',\n",
       " 'ㄴ',\n",
       " '1',\n",
       " '인',\n",
       " '가구',\n",
       " '지원',\n",
       " '사업',\n",
       " '으로',\n",
       " ',',\n",
       " '경기도',\n",
       " '와',\n",
       " '시',\n",
       " '군',\n",
       " '포함',\n",
       " '하',\n",
       " '어서',\n",
       " '10',\n",
       " '억',\n",
       " '원',\n",
       " '의',\n",
       " '예산',\n",
       " '을',\n",
       " '세우',\n",
       " '어서',\n",
       " '올해',\n",
       " '는',\n",
       " '성남',\n",
       " ',',\n",
       " '안산',\n",
       " ',',\n",
       " '광명',\n",
       " ',',\n",
       " '군포',\n",
       " ',',\n",
       " '포',\n",
       " '천',\n",
       " '5',\n",
       " '개',\n",
       " '시',\n",
       " '군',\n",
       " '에서',\n",
       " '3',\n",
       " '월',\n",
       " '부터',\n",
       " '시범',\n",
       " '적',\n",
       " '으로',\n",
       " '운영',\n",
       " '하',\n",
       " 'ㄹ',\n",
       " '계획',\n",
       " '이',\n",
       " 'ㅂ니다',\n",
       " '.',\n",
       " '1',\n",
       " '이',\n",
       " 'ㄴ가',\n",
       " '굴',\n",
       " '는',\n",
       " '아프',\n",
       " 'ㄹ',\n",
       " '때',\n",
       " '더',\n",
       " '서럽',\n",
       " '다는',\n",
       " '말',\n",
       " '이',\n",
       " '있',\n",
       " '는데요',\n",
       " '.',\n",
       " '이',\n",
       " '사업',\n",
       " '은',\n",
       " '혼자',\n",
       " '병원',\n",
       " '방문',\n",
       " '이',\n",
       " '어렵',\n",
       " 'ㄴ',\n",
       " '1',\n",
       " '인',\n",
       " '가구',\n",
       " '를',\n",
       " '위하',\n",
       " '어',\n",
       " '요양',\n",
       " '보호',\n",
       " '사',\n",
       " '등',\n",
       " '의',\n",
       " '자격',\n",
       " '을',\n",
       " '가지',\n",
       " 'ㄴ',\n",
       " '동행인',\n",
       " '이',\n",
       " '병원',\n",
       " '출발',\n",
       " '과',\n",
       " '귀가',\n",
       " '시',\n",
       " '동행',\n",
       " '하',\n",
       " '어',\n",
       " '드리고',\n",
       " ',',\n",
       " '병원',\n",
       " '접수',\n",
       " '와',\n",
       " '수납',\n",
       " '까지',\n",
       " '지원',\n",
       " '하',\n",
       " '어',\n",
       " '드리',\n",
       " '는',\n",
       " '사업',\n",
       " '이',\n",
       " 'ㅂ니다',\n",
       " '.',\n",
       " '◇',\n",
       " '\\xa0',\n",
       " '이',\n",
       " '현웅',\n",
       " ':',\n",
       " '이',\n",
       " '사업',\n",
       " '을',\n",
       " '추진',\n",
       " '하',\n",
       " '게',\n",
       " '되',\n",
       " 'ㄴ',\n",
       " '계기',\n",
       " '는',\n",
       " '무엇',\n",
       " '이',\n",
       " 'ㄴ가요',\n",
       " '?',\n",
       " '◆',\n",
       " '최',\n",
       " '영',\n",
       " '묵',\n",
       " ':',\n",
       " '2021',\n",
       " '년',\n",
       " '말',\n",
       " '기준',\n",
       " '으로',\n",
       " '경기도',\n",
       " '에서',\n",
       " '는',\n",
       " '1',\n",
       " '이',\n",
       " 'ㄴ가',\n",
       " '구가',\n",
       " '154',\n",
       " '만',\n",
       " '가구',\n",
       " '가',\n",
       " '살',\n",
       " '고',\n",
       " '있',\n",
       " '는데요',\n",
       " '.',\n",
       " '이',\n",
       " '는',\n",
       " '경기도',\n",
       " '전체',\n",
       " '가구',\n",
       " '의',\n",
       " '29.2',\n",
       " '%',\n",
       " '로',\n",
       " '10',\n",
       " '가구',\n",
       " '중',\n",
       " '3',\n",
       " '가구',\n",
       " '가',\n",
       " '1',\n",
       " '이',\n",
       " 'ㄴ가',\n",
       " '구인',\n",
       " '셈',\n",
       " '이',\n",
       " '죠',\n",
       " '.',\n",
       " '또',\n",
       " '전국',\n",
       " '의',\n",
       " '몇',\n",
       " '21.5',\n",
       " '%',\n",
       " '로',\n",
       " '경기도',\n",
       " '에',\n",
       " '가장',\n",
       " '많',\n",
       " '은',\n",
       " '1',\n",
       " '이',\n",
       " 'ㄴ가',\n",
       " '구가',\n",
       " '거주',\n",
       " '하',\n",
       " '고',\n",
       " '있',\n",
       " '습니다',\n",
       " '.',\n",
       " '저희',\n",
       " '가',\n",
       " '2021',\n",
       " '년',\n",
       " '실시',\n",
       " '하',\n",
       " 'ㄴ',\n",
       " '「',\n",
       " '경기도',\n",
       " '1',\n",
       " '인',\n",
       " '가구',\n",
       " '실태',\n",
       " '조사',\n",
       " '」',\n",
       " '결과',\n",
       " '*',\n",
       " ',',\n",
       " '몸',\n",
       " '이',\n",
       " '아프',\n",
       " 'ㄹ',\n",
       " '때',\n",
       " '대처',\n",
       " '의',\n",
       " '어려움',\n",
       " '이',\n",
       " '있',\n",
       " '다고',\n",
       " '응답',\n",
       " '하',\n",
       " 'ㄴ',\n",
       " '비율',\n",
       " '이',\n",
       " '29.6',\n",
       " '%',\n",
       " '로',\n",
       " ',',\n",
       " '도내',\n",
       " '1',\n",
       " '인',\n",
       " '가구',\n",
       " '증가',\n",
       " '및',\n",
       " '삶',\n",
       " '의',\n",
       " '질',\n",
       " '향상',\n",
       " '을',\n",
       " '위하',\n",
       " 'ㄴ',\n",
       " '정책',\n",
       " '지원',\n",
       " '이',\n",
       " '필요',\n",
       " '하다',\n",
       " '이',\n",
       " '고',\n",
       " '판단',\n",
       " '하',\n",
       " '여',\n",
       " '이',\n",
       " '사업',\n",
       " '을',\n",
       " '계획',\n",
       " '하',\n",
       " '게',\n",
       " '되',\n",
       " '었',\n",
       " '습니다',\n",
       " '.',\n",
       " '◇',\n",
       " '\\xa0',\n",
       " '이',\n",
       " '현웅',\n",
       " ':',\n",
       " '1',\n",
       " '인',\n",
       " '가구',\n",
       " '이',\n",
       " '라면',\n",
       " '누구',\n",
       " '나',\n",
       " '신청',\n",
       " '하',\n",
       " 'ㄹ',\n",
       " '수',\n",
       " '있',\n",
       " '는',\n",
       " '것',\n",
       " '이',\n",
       " 'ㄴ지',\n",
       " '궁금',\n",
       " '하',\n",
       " '네요',\n",
       " '.',\n",
       " '사업',\n",
       " '의',\n",
       " '대상자',\n",
       " '와',\n",
       " '신청',\n",
       " '방법',\n",
       " ',',\n",
       " '또',\n",
       " '이용',\n",
       " '요금',\n",
       " '은',\n",
       " '어떻',\n",
       " '게',\n",
       " '되',\n",
       " 'ㄹ까요',\n",
       " '?',\n",
       " '◆',\n",
       " '최',\n",
       " '영',\n",
       " '묵',\n",
       " ':',\n",
       " '사업',\n",
       " '대상',\n",
       " '은',\n",
       " '5',\n",
       " '개',\n",
       " '시',\n",
       " '군',\n",
       " '내',\n",
       " '1',\n",
       " '이',\n",
       " 'ㄴ가',\n",
       " '구로',\n",
       " ',',\n",
       " '연령',\n",
       " ',',\n",
       " '소득',\n",
       " '과',\n",
       " '무관',\n",
       " '하',\n",
       " '게',\n",
       " '병원',\n",
       " '이용',\n",
       " '에',\n",
       " '어려움',\n",
       " '이',\n",
       " '있',\n",
       " '는',\n",
       " '사실상',\n",
       " '1',\n",
       " '인',\n",
       " '가구',\n",
       " '는',\n",
       " '누구',\n",
       " '나',\n",
       " '신청',\n",
       " '하',\n",
       " '시',\n",
       " 'ㄹ',\n",
       " '수',\n",
       " '있',\n",
       " '습니다',\n",
       " '.',\n",
       " '하',\n",
       " '어',\n",
       " '당',\n",
       " '시',\n",
       " '군',\n",
       " '에',\n",
       " '전화',\n",
       " '예약',\n",
       " '을',\n",
       " '통하',\n",
       " '어',\n",
       " '신청',\n",
       " '가능',\n",
       " '하',\n",
       " '며',\n",
       " ',',\n",
       " '기본',\n",
       " '1',\n",
       " '시간',\n",
       " '당',\n",
       " '5,000',\n",
       " '원',\n",
       " ',',\n",
       " '초과',\n",
       " '30',\n",
       " '분당',\n",
       " '2,500',\n",
       " '원',\n",
       " '소',\n",
       " '정의',\n",
       " '이용료',\n",
       " '로',\n",
       " '이용',\n",
       " '하',\n",
       " 'ㄹ',\n",
       " '수',\n",
       " '있',\n",
       " '습니다',\n",
       " '.',\n",
       " '다만',\n",
       " ',',\n",
       " '시',\n",
       " '군',\n",
       " '별로',\n",
       " '사업',\n",
       " '개시일',\n",
       " '이',\n",
       " '다르',\n",
       " '니',\n",
       " '자세',\n",
       " '하',\n",
       " 'ㄴ',\n",
       " '사항',\n",
       " '은',\n",
       " '시',\n",
       " '군',\n",
       " '에',\n",
       " '문의',\n",
       " '하',\n",
       " '시',\n",
       " '기',\n",
       " '바라',\n",
       " 'ㅂ니다',\n",
       " '.',\n",
       " '◇',\n",
       " '\\xa0',\n",
       " '이',\n",
       " '현웅',\n",
       " ':',\n",
       " '서울시',\n",
       " '에서',\n",
       " '도',\n",
       " '이',\n",
       " '사업',\n",
       " '을',\n",
       " '운영',\n",
       " '하',\n",
       " '고',\n",
       " '있',\n",
       " '는',\n",
       " '것',\n",
       " '으로',\n",
       " '알',\n",
       " '고',\n",
       " '있',\n",
       " '는데',\n",
       " ',',\n",
       " '경기도',\n",
       " '와',\n",
       " '서울시',\n",
       " '사업',\n",
       " '의',\n",
       " '차이점',\n",
       " '은',\n",
       " '무엇',\n",
       " '이',\n",
       " 'ㄴ가요',\n",
       " '?',\n",
       " '◆',\n",
       " '최',\n",
       " '영',\n",
       " '묵',\n",
       " ':',\n",
       " '병원',\n",
       " '동행',\n",
       " '서비스',\n",
       " '지원',\n",
       " '내용',\n",
       " '은',\n",
       " '동일',\n",
       " '하',\n",
       " 'ㅂ니다',\n",
       " '.',\n",
       " '다만',\n",
       " '서울시',\n",
       " '는',\n",
       " '도시형',\n",
       " '이',\n",
       " '기에',\n",
       " '일괄적',\n",
       " '이',\n",
       " 'ㄴ',\n",
       " '사업',\n",
       " '추진',\n",
       " '이',\n",
       " '가능',\n",
       " '하',\n",
       " 'ㄴ',\n",
       " '반면',\n",
       " ',',\n",
       " '경기도',\n",
       " '는',\n",
       " '도시형',\n",
       " '부터',\n",
       " '농촌',\n",
       " '형',\n",
       " ',',\n",
       " '도',\n",
       " '농',\n",
       " '복합형',\n",
       " '등',\n",
       " '다양',\n",
       " '하',\n",
       " 'ㄴ',\n",
       " '지역적',\n",
       " '특성',\n",
       " '을',\n",
       " '가지',\n",
       " '고',\n",
       " '있',\n",
       " '기에',\n",
       " '최대한',\n",
       " '지역',\n",
       " '특성',\n",
       " '과',\n",
       " '욕구',\n",
       " '가',\n",
       " '반영',\n",
       " '되',\n",
       " 'ㄹ',\n",
       " '수',\n",
       " '있',\n",
       " '도록',\n",
       " '시',\n",
       " '군',\n",
       " '과',\n",
       " '세부적',\n",
       " '으로',\n",
       " '협의',\n",
       " '하',\n",
       " '면서',\n",
       " '점차',\n",
       " '확대',\n",
       " '운영',\n",
       " '하',\n",
       " 'ㄴ다는',\n",
       " '것',\n",
       " '이',\n",
       " '차이점',\n",
       " '이',\n",
       " '라고',\n",
       " '보',\n",
       " 'ㄹ',\n",
       " '수',\n",
       " '있',\n",
       " '습니다',\n",
       " '.',\n",
       " '◇',\n",
       " '\\xa0',\n",
       " '이',\n",
       " '현웅',\n",
       " ':',\n",
       " '지금',\n",
       " '까지',\n",
       " '경기도',\n",
       " '1',\n",
       " '인',\n",
       " '가구',\n",
       " '병원',\n",
       " '안심',\n",
       " '동행',\n",
       " '사업',\n",
       " '에',\n",
       " '대하',\n",
       " '어',\n",
       " '알아보',\n",
       " '았',\n",
       " '는데요',\n",
       " ',',\n",
       " '마지막',\n",
       " '마무리',\n",
       " '말씀',\n",
       " '부탁',\n",
       " '드리',\n",
       " 'ㅂ니다',\n",
       " '.',\n",
       " '◆',\n",
       " '최',\n",
       " '영',\n",
       " '묵',\n",
       " ':',\n",
       " '경기도',\n",
       " '에서',\n",
       " '는',\n",
       " '작년',\n",
       " '12',\n",
       " '월',\n",
       " '「',\n",
       " '1',\n",
       " '인',\n",
       " '가구',\n",
       " '정책',\n",
       " '5',\n",
       " '개년',\n",
       " '기본',\n",
       " '계획',\n",
       " '(',\n",
       " '’',\n",
       " '23',\n",
       " '~',\n",
       " '’',\n",
       " '27',\n",
       " ')',\n",
       " '」',\n",
       " '을',\n",
       " '수립',\n",
       " '하',\n",
       " '였',\n",
       " '고',\n",
       " ',',\n",
       " '적극적',\n",
       " '이',\n",
       " 'ㄴ',\n",
       " '1',\n",
       " '인',\n",
       " '가구',\n",
       " '사업',\n",
       " '추진',\n",
       " '을',\n",
       " '위하',\n",
       " '어',\n",
       " '‘',\n",
       " '1',\n",
       " '인',\n",
       " '가구',\n",
       " '지원',\n",
       " '팀',\n",
       " '’',\n",
       " '도',\n",
       " '신설',\n",
       " '하',\n",
       " '였',\n",
       " '습니다',\n",
       " '.',\n",
       " '그리고',\n",
       " '병원',\n",
       " '안심',\n",
       " '동행',\n",
       " '사업',\n",
       " '외',\n",
       " '에',\n",
       " '도',\n",
       " '신규',\n",
       " '사업',\n",
       " '으로',\n",
       " '‘',\n",
       " '여성',\n",
       " '1',\n",
       " '인',\n",
       " '가구',\n",
       " '안심',\n",
       " '패키지',\n",
       " '보급',\n",
       " '사업',\n",
       " '’',\n",
       " ',',\n",
       " '‘',\n",
       " '1',\n",
       " '인',\n",
       " '가구',\n",
       " '자유',\n",
       " '주제',\n",
       " '’',\n",
       " '사업',\n",
       " '등',\n",
       " '도',\n",
       " '추진',\n",
       " '예정',\n",
       " '이',\n",
       " 'ㅂ니다',\n",
       " '.',\n",
       " '1',\n",
       " '인',\n",
       " '가구',\n",
       " '에게',\n",
       " '도',\n",
       " '더',\n",
       " '낫',\n",
       " '은',\n",
       " '기회',\n",
       " '를',\n",
       " '제공',\n",
       " '하',\n",
       " '고',\n",
       " '‘',\n",
       " '1',\n",
       " '인',\n",
       " '가구',\n",
       " '에',\n",
       " '힘',\n",
       " '이',\n",
       " '되',\n",
       " '는',\n",
       " '경기도',\n",
       " '’',\n",
       " '를',\n",
       " '만들',\n",
       " '기',\n",
       " '위하',\n",
       " '어',\n",
       " '노력',\n",
       " '하',\n",
       " '겠',\n",
       " '습니다',\n",
       " '.',\n",
       " '감사',\n",
       " '하',\n",
       " 'ㅂ니다',\n",
       " '.',\n",
       " '◇',\n",
       " '\\xa0',\n",
       " '이',\n",
       " '현웅',\n",
       " ':',\n",
       " ...]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = {\n",
    "    'text': data['text'][:100],\n",
    "    'tokens': data['tokens'][:100],\n",
    "    'pos': data['pos'][:100]\n",
    "    }\n",
    "test_data['tokens'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'JKO': [[0.10294117647058823, 0.29411764705882354, 0.75]],\n",
       " 'JX': [[0.04411764705882353, 0.17647058823529413, 0.5, 0.5588235294117647]],\n",
       " 'EFN': [[0.9705882352941176]],\n",
       " 'JKG': [[0.47058823529411764, 0.5882352941176471]],\n",
       " 'SF': [[0.9852941176470589]],\n",
       " 'XSV': [[0.1323529411764706, 0.7794117647058824, 0.8970588235294118]],\n",
       " 'ECS': [[0.3235294117647059, 0.6911764705882353, 0.8676470588235294]],\n",
       " 'MDT': [[0.4411764705882353, 0.6029411764705882, 0.7205882352941176]],\n",
       " 'VXV': [[0.3382352941176471, 0.8088235294117647]],\n",
       " 'SP': [[0.25, 0.27941176470588236, 0.4264705882352941, 0.7058823529411765]],\n",
       " 'JKM': [[0.029411764705882353, 0.5441176470588235, 0.6323529411764706]],\n",
       " 'ECE': [[0.4117647058823529, 0.7941176470588235]],\n",
       " 'EPT': [[0.9117647058823529]],\n",
       " 'VV': [[0.20588235294117646,\n",
       "   0.3088235294117647,\n",
       "   0.6764705882352942,\n",
       "   0.8529411764705882]],\n",
       " 'NNG': [[0.014705882352941176,\n",
       "   0.058823529411764705,\n",
       "   0.07352941176470588,\n",
       "   0.08823529411764706,\n",
       "   0.11764705882352941,\n",
       "   0.16176470588235295,\n",
       "   0.23529411764705882,\n",
       "   0.2647058823529412,\n",
       "   0.36764705882352944,\n",
       "   0.45588235294117646,\n",
       "   0.4852941176470588,\n",
       "   0.5147058823529411,\n",
       "   0.5294117647058824,\n",
       "   0.5735294117647058,\n",
       "   0.6176470588235294,\n",
       "   0.6470588235294118,\n",
       "   0.7352941176470589,\n",
       "   0.7647058823529411,\n",
       "   0.8382352941176471,\n",
       "   0.8823529411764706,\n",
       "   0.9411764705882353]],\n",
       " 'MAG': [[0.0, 0.19117647058823528]],\n",
       " 'ETD': [[0.14705882352941177,\n",
       "   0.22058823529411764,\n",
       "   0.35294117647058826,\n",
       "   0.8235294117647058,\n",
       "   0.9264705882352942]],\n",
       " 'JKS': [[0.38235294117647056, 0.6617647058823529]],\n",
       " 'VA': [[0.39705882352941174]],\n",
       " 'VCP': [[0.9558823529411765]]}"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_token_sentences = np.array(new_analysis['num_token_sentences'])\n",
    "sentence_pos = data['sentence_pos']\n",
    "calRelPosition(sentence_pos, num_token_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4       , 0.58823529, 0.57142857, 0.31111111, 0.41463415,\n",
       "       0.58333333, 0.29411765])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calTokenVariety(new_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['음식', 'NNG'],\n",
       " ['을', 'JKO'],\n",
       " ['씹', 'VV'],\n",
       " ['기', 'ETN'],\n",
       " ['어렵', 'VA'],\n",
       " ['ㄴ', 'ETD'],\n",
       " ['노인', 'NNG'],\n",
       " ['은', 'JX'],\n",
       " ['노쇠', 'NNG'],\n",
       " ['위험', 'NNG'],\n",
       " ['이', 'JKS'],\n",
       " ['2.7', 'NR'],\n",
       " ['배', 'NNG'],\n",
       " ['높', 'VA'],\n",
       " ['아', 'ECD'],\n",
       " ['노년기', 'NNG'],\n",
       " ['에', 'JKM'],\n",
       " ['급격', 'XR'],\n",
       " ['하', 'XSA'],\n",
       " ['ㄴ', 'ETD'],\n",
       " ['노쇠', 'NNG'],\n",
       " ['를', 'JKO'],\n",
       " ['막', 'VV'],\n",
       " ['으려', 'ECD'],\n",
       " ['면', 'NNG'],\n",
       " ['평소', 'NNG'],\n",
       " ['치아', 'NNG'],\n",
       " ['건강', 'NNG'],\n",
       " ['을', 'JKO'],\n",
       " ['유지', 'NNG'],\n",
       " ['하', 'XSV'],\n",
       " ['는', 'ETD'],\n",
       " ['것', 'NNB'],\n",
       " ['이', 'JKS'],\n",
       " ['중요', 'NNG'],\n",
       " ['하', 'XSV'],\n",
       " ['ㄴ', 'ETD'],\n",
       " ['것', 'NNB'],\n",
       " ['으로', 'JKM'],\n",
       " ['나타나', 'VV'],\n",
       " ['었', 'EPT'],\n",
       " ['습니다', 'EFN'],\n",
       " ['.', 'SF'],\n",
       " ['서울', 'NNG'],\n",
       " ['아산', 'NNP'],\n",
       " ['병원', 'NNG'],\n",
       " ['노년', 'NNG'],\n",
       " ['내과', 'NNG'],\n",
       " ['정', 'NNG'],\n",
       " ['희원', 'NNG'],\n",
       " [',', 'SP'],\n",
       " ['빛', 'NNG'],\n",
       " ['고을', 'NNG'],\n",
       " ['전', 'NNG'],\n",
       " ['남대', 'NNG'],\n",
       " ['병원', 'NNG'],\n",
       " ['노년', 'NNG'],\n",
       " ['내과', 'NNG'],\n",
       " ['강', 'NNG'],\n",
       " ['민', 'NNG'],\n",
       " ['구', 'NNG'],\n",
       " ['교수', 'NNG'],\n",
       " ['공동', 'NNG'],\n",
       " ['연구', 'NNG'],\n",
       " ['팀', 'NNG'],\n",
       " ['은', 'JX'],\n",
       " ['2016', 'NR'],\n",
       " ['∼', 'SO'],\n",
       " ['2018', 'NR'],\n",
       " ['년', 'NNB'],\n",
       " ['국민', 'NNG'],\n",
       " ['건강', 'NNG'],\n",
       " ['영양', 'NNG'],\n",
       " ['조사', 'NNG'],\n",
       " ['에', 'JKM'],\n",
       " ['참여', 'NNG'],\n",
       " ['하', 'XSV'],\n",
       " ['ㄴ', 'ETD'],\n",
       " ['65', 'NR'],\n",
       " ['세', 'NNM'],\n",
       " ['이상', 'NNG'],\n",
       " ['3', 'NR'],\n",
       " ['천', 'NR'],\n",
       " ['여', 'NR'],\n",
       " ['명', 'NNM'],\n",
       " ['을', 'JKO'],\n",
       " ['대상', 'NNG'],\n",
       " ['으로', 'JKM'],\n",
       " ['노년기', 'NNG'],\n",
       " ['이후', 'NNG'],\n",
       " ['음식', 'NNG'],\n",
       " ['을', 'JKO'],\n",
       " ['씹', 'VV'],\n",
       " ['는', 'ETD'],\n",
       " ['저작', 'NNG'],\n",
       " ['기능', 'NNG'],\n",
       " ['이', 'JKS'],\n",
       " ['노쇠', 'NNG'],\n",
       " ['에', 'JKM'],\n",
       " ['미치', 'VV'],\n",
       " ['는', 'ETD'],\n",
       " ['영향', 'NNG'],\n",
       " ['을', 'JKO'],\n",
       " ['평가', 'NNG'],\n",
       " ['하', 'XSV'],\n",
       " ['었', 'EPT'],\n",
       " ['습니다', 'EFN'],\n",
       " ['.', 'SF'],\n",
       " ['노쇠', 'NNG'],\n",
       " ['는', 'JX'],\n",
       " ['신체', 'NNG'],\n",
       " ['기능', 'NNG'],\n",
       " ['이', 'JKS'],\n",
       " ['급격히', 'MAG'],\n",
       " ['허약', 'NNG'],\n",
       " ['해지', 'VV'],\n",
       " ['어', 'ECS'],\n",
       " ['장애', 'NNG'],\n",
       " ['나', 'JC'],\n",
       " ['입원', 'NNG'],\n",
       " ['가능성', 'NNG'],\n",
       " ['이', 'JKS'],\n",
       " ['커지', 'VV'],\n",
       " ['ㄴ', 'ETD'],\n",
       " ['상태', 'NNG'],\n",
       " ['를', 'JKO'],\n",
       " ['말하', 'VV'],\n",
       " ['ㅂ니다', 'EFN'],\n",
       " ['.', 'SF'],\n",
       " ['그', 'MDT'],\n",
       " ['결과', 'NNG'],\n",
       " ['평소', 'NNG'],\n",
       " ['음식', 'NNG'],\n",
       " ['을', 'JKO'],\n",
       " ['씹', 'VV'],\n",
       " ['는데', 'ECD'],\n",
       " ['어려움', 'NNG'],\n",
       " ['을', 'JKO'],\n",
       " ['호소', 'NNG'],\n",
       " ['하', 'XSV'],\n",
       " ['는', 'ETD'],\n",
       " ['노인', 'NNG'],\n",
       " ['은', 'JX'],\n",
       " ['그렇', 'VA'],\n",
       " ['지', 'ECD'],\n",
       " ['않', 'VXV'],\n",
       " ['은', 'ETD'],\n",
       " ['노인', 'NNG'],\n",
       " ['보다', 'JKM'],\n",
       " ['노쇠', 'NNG'],\n",
       " ['위험', 'NNG'],\n",
       " ['이', 'JKS'],\n",
       " ['2.68', 'NR'],\n",
       " ['배', 'NNG'],\n",
       " ['높', 'VA'],\n",
       " ['은', 'ETD'],\n",
       " ['것', 'NNB'],\n",
       " ['으로', 'JKM'],\n",
       " ['분석', 'NNG'],\n",
       " ['되', 'XSV'],\n",
       " ['었', 'EPT'],\n",
       " ['습니다', 'EFN'],\n",
       " ['.', 'SF'],\n",
       " ['연구', 'NNG'],\n",
       " ['팀', 'NNG'],\n",
       " ['은', 'JX'],\n",
       " ['음식', 'NNG'],\n",
       " ['을', 'JKO'],\n",
       " ['씹', 'VV'],\n",
       " ['는', 'ETD'],\n",
       " ['능력', 'NNG'],\n",
       " ['이', 'JKS'],\n",
       " ['영양', 'NNG'],\n",
       " ['섭취', 'NNG'],\n",
       " ['와', 'JKM'],\n",
       " ['식단', 'NNG'],\n",
       " ['선택', 'NNG'],\n",
       " ['에', 'JKM'],\n",
       " ['크', 'VA'],\n",
       " ['ㄴ', 'ETD'],\n",
       " ['영향', 'NNG'],\n",
       " ['을', 'JKO'],\n",
       " ['끼치', 'VV'],\n",
       " ['기', 'ETN'],\n",
       " ['때문', 'NNB'],\n",
       " ['에', 'JKM'],\n",
       " ['노년기', 'NNG'],\n",
       " ['의', 'JKG'],\n",
       " ['전신', 'NNG'],\n",
       " ['건강', 'NNG'],\n",
       " ['상태', 'NNG'],\n",
       " ['를', 'JKO'],\n",
       " ['파악', 'NNG'],\n",
       " ['하', 'XSV'],\n",
       " ['는', 'ETD'],\n",
       " ['지표', 'NNG'],\n",
       " ['가', 'JKC'],\n",
       " ['되', 'VV'],\n",
       " ['ㄹ', 'ETD'],\n",
       " ['수', 'NNB'],\n",
       " ['있', 'VV'],\n",
       " ['다고', 'EFN'],\n",
       " ['설명', 'NNG'],\n",
       " ['하', 'XSV'],\n",
       " ['었', 'EPT'],\n",
       " ['습니다', 'EFN'],\n",
       " ['.', 'SF'],\n",
       " ['정', 'XPN'],\n",
       " ['희원', 'NNG'],\n",
       " ['교수', 'NNG'],\n",
       " ['는', 'JX'],\n",
       " ['건강', 'NNG'],\n",
       " ['하', 'XSV'],\n",
       " ['ㄴ', 'ETD'],\n",
       " ['노년', 'NNG'],\n",
       " ['을', 'JKO'],\n",
       " ['맞', 'VV'],\n",
       " ['으려', 'ECD'],\n",
       " ['면', 'NNG'],\n",
       " ['평소', 'NNG'],\n",
       " ['치아', 'NNG'],\n",
       " ['상태', 'NNG'],\n",
       " ['를', 'JKO'],\n",
       " ['건강', 'NNG'],\n",
       " ['하', 'XSV'],\n",
       " ['게', 'ECD'],\n",
       " ['관리', 'NNG'],\n",
       " ['하', 'XSV'],\n",
       " ['는', 'ETD'],\n",
       " ['것', 'NNB'],\n",
       " ['이', 'JKS'],\n",
       " ['중요', 'NNG'],\n",
       " ['하다', 'NNP'],\n",
       " ['이', 'VCP'],\n",
       " ['면', 'ECE'],\n",
       " ['스', 'VV'],\n",
       " ['어', 'ECS'],\n",
       " ['음식', 'NNG'],\n",
       " ['을', 'JKO'],\n",
       " ['씹', 'VV'],\n",
       " ['는', 'ETD'],\n",
       " ['데', 'NNB'],\n",
       " ['어려움', 'NNG'],\n",
       " ['이', 'JKS'],\n",
       " ['있', 'VA'],\n",
       " ['다면', 'ECE'],\n",
       " ['고령', 'NNG'],\n",
       " ['친화', 'NNG'],\n",
       " ['식품', 'NNG'],\n",
       " ['이나', 'JC'],\n",
       " ['보충제', 'NNG'],\n",
       " ['등', 'NNB'],\n",
       " ['을', 'JKO'],\n",
       " ['통하', 'VV'],\n",
       " ['어', 'ECS'],\n",
       " ['영양분', 'NNG'],\n",
       " ['을', 'JKO'],\n",
       " ['골고루', 'MAG'],\n",
       " ['섭취', 'NNG'],\n",
       " ['하', 'XSV'],\n",
       " ['어', 'ECS'],\n",
       " ['노쇠', 'NNG'],\n",
       " ['를', 'JKO'],\n",
       " ['예방', 'NNG'],\n",
       " ['하', 'XSV'],\n",
       " ['려는', 'ETD'],\n",
       " ['노력', 'NNG'],\n",
       " ['이', 'JKS'],\n",
       " ['필요', 'NNG'],\n",
       " ['하다', 'NNP'],\n",
       " ['이', 'VCP'],\n",
       " ['고', 'ECE'],\n",
       " ['말하', 'VV'],\n",
       " ['었', 'EPT'],\n",
       " ['습니다', 'EFN'],\n",
       " ['.', 'SF'],\n",
       " ['이번', 'NNG'],\n",
       " ['연구', 'NNG'],\n",
       " ['결과', 'NNG'],\n",
       " ['는', 'JX'],\n",
       " ['국제', 'NNG'],\n",
       " ['학술지', 'NNG'],\n",
       " [\"'\", 'SS'],\n",
       " ['노년', 'NNG'],\n",
       " ['임상', 'NNG'],\n",
       " ['중재', 'NNG'],\n",
       " [\"'\", 'SS'],\n",
       " ['(', 'SS'],\n",
       " ['Clinical', 'OL'],\n",
       " ['Interventions', 'OL'],\n",
       " ['in', 'OL'],\n",
       " ['Aging', 'OL'],\n",
       " [')', 'SS'],\n",
       " ['최근', 'NNG'],\n",
       " ['호', 'NNG'],\n",
       " ['에', 'JKM'],\n",
       " ['게재', 'NNG'],\n",
       " ['되', 'XSV'],\n",
       " ['었', 'EPT'],\n",
       " ['습니다', 'EFN'],\n",
       " ['.', 'SF']]"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NNG</th>\n",
       "      <th>JKO</th>\n",
       "      <th>VV</th>\n",
       "      <th>ETD</th>\n",
       "      <th>JX</th>\n",
       "      <th>MAG</th>\n",
       "      <th>VA</th>\n",
       "      <th>EFN</th>\n",
       "      <th>SF</th>\n",
       "      <th>JKM</th>\n",
       "      <th>...</th>\n",
       "      <th>IC</th>\n",
       "      <th>EFA</th>\n",
       "      <th>JKI</th>\n",
       "      <th>EFO</th>\n",
       "      <th>MAC</th>\n",
       "      <th>OL</th>\n",
       "      <th>JKQ</th>\n",
       "      <th>SE</th>\n",
       "      <th>EPP</th>\n",
       "      <th>XPV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.195440</td>\n",
       "      <td>0.028891</td>\n",
       "      <td>0.125620</td>\n",
       "      <td>0.049851</td>\n",
       "      <td>0.047869</td>\n",
       "      <td>0.043337</td>\n",
       "      <td>0.022093</td>\n",
       "      <td>0.062031</td>\n",
       "      <td>0.085824</td>\n",
       "      <td>0.032857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000850</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000283</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.194787</td>\n",
       "      <td>0.037723</td>\n",
       "      <td>0.127915</td>\n",
       "      <td>0.068244</td>\n",
       "      <td>0.036008</td>\n",
       "      <td>0.038752</td>\n",
       "      <td>0.022634</td>\n",
       "      <td>0.032236</td>\n",
       "      <td>0.041495</td>\n",
       "      <td>0.035665</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002401</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.234636</td>\n",
       "      <td>0.036160</td>\n",
       "      <td>0.125993</td>\n",
       "      <td>0.056429</td>\n",
       "      <td>0.040701</td>\n",
       "      <td>0.046862</td>\n",
       "      <td>0.022864</td>\n",
       "      <td>0.048808</td>\n",
       "      <td>0.058537</td>\n",
       "      <td>0.047511</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.238231</td>\n",
       "      <td>0.025963</td>\n",
       "      <td>0.108131</td>\n",
       "      <td>0.040799</td>\n",
       "      <td>0.061626</td>\n",
       "      <td>0.032240</td>\n",
       "      <td>0.013695</td>\n",
       "      <td>0.040799</td>\n",
       "      <td>0.055350</td>\n",
       "      <td>0.042225</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001997</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.000856</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.207165</td>\n",
       "      <td>0.036916</td>\n",
       "      <td>0.122890</td>\n",
       "      <td>0.051609</td>\n",
       "      <td>0.045780</td>\n",
       "      <td>0.037158</td>\n",
       "      <td>0.023072</td>\n",
       "      <td>0.052216</td>\n",
       "      <td>0.064117</td>\n",
       "      <td>0.038494</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>0.000729</td>\n",
       "      <td>0.000729</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.180974</td>\n",
       "      <td>0.037826</td>\n",
       "      <td>0.157418</td>\n",
       "      <td>0.041450</td>\n",
       "      <td>0.043262</td>\n",
       "      <td>0.048018</td>\n",
       "      <td>0.025595</td>\n",
       "      <td>0.041223</td>\n",
       "      <td>0.050057</td>\n",
       "      <td>0.027860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000680</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000680</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.157643</td>\n",
       "      <td>0.030255</td>\n",
       "      <td>0.112527</td>\n",
       "      <td>0.044055</td>\n",
       "      <td>0.045648</td>\n",
       "      <td>0.032909</td>\n",
       "      <td>0.027070</td>\n",
       "      <td>0.066879</td>\n",
       "      <td>0.079618</td>\n",
       "      <td>0.029724</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004246</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.200289</td>\n",
       "      <td>0.038410</td>\n",
       "      <td>0.106992</td>\n",
       "      <td>0.065961</td>\n",
       "      <td>0.048628</td>\n",
       "      <td>0.030867</td>\n",
       "      <td>0.022040</td>\n",
       "      <td>0.038464</td>\n",
       "      <td>0.040282</td>\n",
       "      <td>0.043546</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000642</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.207422</td>\n",
       "      <td>0.031626</td>\n",
       "      <td>0.109200</td>\n",
       "      <td>0.068294</td>\n",
       "      <td>0.045427</td>\n",
       "      <td>0.030657</td>\n",
       "      <td>0.021864</td>\n",
       "      <td>0.038700</td>\n",
       "      <td>0.058338</td>\n",
       "      <td>0.036507</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000582</td>\n",
       "      <td>0.000635</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.196629</td>\n",
       "      <td>0.030431</td>\n",
       "      <td>0.135300</td>\n",
       "      <td>0.033240</td>\n",
       "      <td>0.046504</td>\n",
       "      <td>0.041979</td>\n",
       "      <td>0.012484</td>\n",
       "      <td>0.040574</td>\n",
       "      <td>0.049469</td>\n",
       "      <td>0.025281</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000624</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.000624</td>\n",
       "      <td>0.000780</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.001092</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          NNG       JKO        VV       ETD        JX       MAG        VA  \\\n",
       "0    0.195440  0.028891  0.125620  0.049851  0.047869  0.043337  0.022093   \n",
       "1    0.194787  0.037723  0.127915  0.068244  0.036008  0.038752  0.022634   \n",
       "2    0.234636  0.036160  0.125993  0.056429  0.040701  0.046862  0.022864   \n",
       "3    0.238231  0.025963  0.108131  0.040799  0.061626  0.032240  0.013695   \n",
       "4    0.207165  0.036916  0.122890  0.051609  0.045780  0.037158  0.023072   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "103  0.180974  0.037826  0.157418  0.041450  0.043262  0.048018  0.025595   \n",
       "104  0.157643  0.030255  0.112527  0.044055  0.045648  0.032909  0.027070   \n",
       "105  0.200289  0.038410  0.106992  0.065961  0.048628  0.030867  0.022040   \n",
       "106  0.207422  0.031626  0.109200  0.068294  0.045427  0.030657  0.021864   \n",
       "107  0.196629  0.030431  0.135300  0.033240  0.046504  0.041979  0.012484   \n",
       "\n",
       "          EFN        SF       JKM  ...        IC       EFA       JKI  \\\n",
       "0    0.062031  0.085824  0.032857  ...  0.000850  0.000142  0.000142   \n",
       "1    0.032236  0.041495  0.035665  ...       NaN  0.000343       NaN   \n",
       "2    0.048808  0.058537  0.047511  ...       NaN       NaN  0.000162   \n",
       "3    0.040799  0.055350  0.042225  ...  0.001997       NaN       NaN   \n",
       "4    0.052216  0.064117  0.038494  ...  0.000486  0.000121  0.000243   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "103  0.041223  0.050057  0.027860  ...  0.000680  0.000227       NaN   \n",
       "104  0.066879  0.079618  0.029724  ...       NaN       NaN       NaN   \n",
       "105  0.038464  0.040282  0.043546  ...       NaN  0.000053       NaN   \n",
       "106  0.038700  0.058338  0.036507  ...  0.000308  0.000087  0.000274   \n",
       "107  0.040574  0.049469  0.025281  ...  0.000624       NaN  0.000156   \n",
       "\n",
       "          EFO       MAC        OL       JKQ        SE       EPP  XPV  \n",
       "0    0.000142  0.000283       NaN       NaN       NaN       NaN  NaN  \n",
       "1         NaN       NaN  0.002401  0.000343       NaN       NaN  NaN  \n",
       "2    0.000162  0.000162  0.000162  0.000162       NaN       NaN  NaN  \n",
       "3    0.000285  0.000856       NaN       NaN       NaN       NaN  NaN  \n",
       "4    0.000729  0.000729       NaN  0.000121       NaN       NaN  NaN  \n",
       "..        ...       ...       ...       ...       ...       ...  ...  \n",
       "103       NaN       NaN  0.000680       NaN       NaN       NaN  NaN  \n",
       "104       NaN       NaN       NaN       NaN  0.004246       NaN  NaN  \n",
       "105  0.000053  0.000642  0.000214  0.000267       NaN       NaN  NaN  \n",
       "106  0.000160  0.000582  0.000635  0.000127       NaN  0.000007  NaN  \n",
       "107  0.000624  0.000780  0.000312  0.001092       NaN       NaN  NaN  \n",
       "\n",
       "[108 rows x 55 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_list = []\n",
    "dic_list = all_analysis['pos_freq']['a_rel_freq']\n",
    "for d in dic_list:\n",
    "    df_list.append(pd.Series(d, dtype = \"float64\"))\n",
    "all_df = pd.concat(df_list, axis = 1).T\n",
    "all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeDF(dic_list):\n",
    "    if type(dic_list) == dict:\n",
    "        return pd.DataFrame(pd.Series(dic_list, dtype = \"float64\")).T             \n",
    "    else:\n",
    "        df_list = []\n",
    "        for d in dic_list:\n",
    "            df_list.append(pd.Series(d, dtype = \"float64\"))\n",
    "        all_df = pd.concat(df_list, axis=1)\n",
    "        return all_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "kkma_dict = {\"NNG\": \"보통 명사\", \n",
    "\"NNP\": \"고유명사\", \n",
    "\"NNB\": \"일반 의존 명사\",\n",
    "\"NNM\": \"단위 의존 명사\",\n",
    "\"NR\": \"수사\",\n",
    "\"NP\": \"대명사\",\n",
    "\"VV\": \"동사\",\n",
    "\"VA\": \"형용사\",\n",
    "\"VXV\": \"보조 동사\",\n",
    "\"VXA\": \"보조 형용사\",\n",
    "\"VCP\": \"긍정 지정사, 서술격 조사 '이다'\",\n",
    "\"VCN\": \"부정 지정사, 형용사 '아니다'\",\n",
    "\"MDT\": \"일반 관형사\",\n",
    "\"MDN\": \"수 관형사\",\n",
    "\"MAG\": \"일반 부사\",\n",
    "\"MAC\": \"접속 부사\",\n",
    "\"IC\": \"감탄사\",\n",
    "\"JKS\": \"주격 조사\",\n",
    "\"JKC\": \"보격 조사\",\n",
    "\"JKG\": \"관형격 조사\",\n",
    "\"JKM\": \"부사격 조사\",\n",
    "\"JKI\": \"호격 조사\",\n",
    "\"JKQ\": \"인용격 조사\",\n",
    "\"JX\": \"보조사\",\n",
    "\"JC\": \"접속 조사\",\n",
    "\"EPH\": \"존칭 선어말 어미\",\n",
    "\"EPT\": \"시제 선어말 어미\",\n",
    "\"EPP\": \"공손 선어말 어미\",\n",
    "\"EFN\": \"평서형 종결 어미\",\n",
    "\"EFQ\": \"의문형 종결 어미\",\n",
    "\"EFO\": \"명령형 종결 어미\",\n",
    "\"EFI\": \"감탄형 종결 어미\",\n",
    "\"EFR\": \"존칭형 종결 어미\",\n",
    "\"ECE\": \"대등 연결 어미\",\n",
    "\"ECD\": \"의존적 연결 어미\",\n",
    "\"ECS\": \"보조적 연결 어미\",\n",
    "\"ETN\": \"명사형 전성 어미\",\n",
    "\"ETD\": \"관형형 전성 어미\",\n",
    "\"XPN\": \"체언 접두사\",\n",
    "\"XPV\": \"용언 접두사\",\n",
    "\"XSN\": \"명사 파생 접미사\",\n",
    "\"XSV\": \"동사 파생 접미사\",\n",
    "\"XSA\": \"형용사 파생 접미사\",\n",
    "\"XR\": \"어근\",\n",
    "\"SF\": \"마침표, 물음표, 느낌표\",\n",
    "\"SP\": \"쉼표, 가운뎃점, 콜론, 빗금\",\n",
    "\"SS\": \"따옴표, 괄호표, 줄표\",\n",
    "\"SE\": \"줄임표\",\n",
    "\"SO\": \"붙임표(물결, 숨김, 빠짐)\",\n",
    "\"SW\": \"기타 기호(논리 수학 기호, 화폐 기호)\",\n",
    "\"UN\": \"명사추정범주\",\n",
    "\"OL\": \"외국어\",\n",
    "\"OH\": \"한자\",\n",
    "\"ON\": \"숫자\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/lhi30/Haein/2023/YBIGTA/2023-2/DA/Writing_Advice/kkma_pos.json\"\n",
    "# path = input(\"Please enter the path to save the analysis result json file\")\n",
    "with open(path, \"w\") as outfile:\n",
    "    json.dump(kkma_dict, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "내가 해야 할 일:\n",
    "- 새로운 토큰화 및 자료 분석 프로세스 만들기\n",
    "변수 종류:\n",
    "- 문장 당 사용한 token 종류 개수 / 문장의 token 수\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hunjangnim",
   "language": "python",
   "name": "hunjangnim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
